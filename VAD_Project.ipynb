{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GmteAhQy9OOM"
   },
   "outputs": [],
   "source": [
    "from utils import create_dataloaders\n",
    "from evaluate import reconstruction_loss, evaluate_model\n",
    "from AutoDecoder import AutoDecoder\n",
    "from trainer import BasicTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_Voa3N89E6w"
   },
   "source": [
    "1.3 Auto Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RTnTKhdwc0-3"
   },
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = create_dataloaders(data_path='', batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQ7uajDHc-7A",
    "outputId": "b6308dfc-9904-4dcb-d932-77382aca3e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/180], Loss: 0.4975\n",
      "Epoch [2/180], Loss: 0.4512\n",
      "Epoch [3/180], Loss: 0.4065\n",
      "Epoch [4/180], Loss: 0.3620\n",
      "Epoch [5/180], Loss: 0.3284\n",
      "Epoch [6/180], Loss: 0.3080\n",
      "Epoch [7/180], Loss: 0.3010\n",
      "Epoch [8/180], Loss: 0.2956\n",
      "Epoch [9/180], Loss: 0.2921\n",
      "Epoch [10/180], Loss: 0.2852\n",
      "Epoch [11/180], Loss: 0.2783\n",
      "Epoch [12/180], Loss: 0.2729\n",
      "Epoch [13/180], Loss: 0.2712\n",
      "Epoch [14/180], Loss: 0.2646\n",
      "Epoch [15/180], Loss: 0.2578\n",
      "Epoch [16/180], Loss: 0.2520\n",
      "Epoch [17/180], Loss: 0.2470\n",
      "Epoch [18/180], Loss: 0.2437\n",
      "Epoch [19/180], Loss: 0.2383\n",
      "Epoch [20/180], Loss: 0.2346\n",
      "Epoch [21/180], Loss: 0.2316\n",
      "Epoch [22/180], Loss: 0.2287\n",
      "Epoch [23/180], Loss: 0.2258\n",
      "Epoch [24/180], Loss: 0.2246\n",
      "Epoch [25/180], Loss: 0.2220\n",
      "Epoch [26/180], Loss: 0.2201\n",
      "Epoch [27/180], Loss: 0.2187\n",
      "Epoch [28/180], Loss: 0.2164\n",
      "Epoch [29/180], Loss: 0.2144\n",
      "Epoch [30/180], Loss: 0.2116\n",
      "Epoch [31/180], Loss: 0.2095\n",
      "Epoch [32/180], Loss: 0.2092\n",
      "Epoch [33/180], Loss: 0.2071\n",
      "Epoch [34/180], Loss: 0.2061\n",
      "Epoch [35/180], Loss: 0.2049\n",
      "Epoch [36/180], Loss: 0.2036\n",
      "Epoch [37/180], Loss: 0.2018\n",
      "Epoch [38/180], Loss: 0.1995\n",
      "Epoch [39/180], Loss: 0.1994\n",
      "Epoch [40/180], Loss: 0.1966\n",
      "Epoch [41/180], Loss: 0.1959\n",
      "Epoch [42/180], Loss: 0.1926\n",
      "Epoch [43/180], Loss: 0.1922\n",
      "Epoch [44/180], Loss: 0.1895\n",
      "Epoch [45/180], Loss: 0.1893\n",
      "Epoch [46/180], Loss: 0.1879\n",
      "Epoch [47/180], Loss: 0.1872\n",
      "Epoch [48/180], Loss: 0.1874\n",
      "Epoch [49/180], Loss: 0.1857\n",
      "Epoch [50/180], Loss: 0.1860\n",
      "Epoch [51/180], Loss: 0.1848\n",
      "Epoch [52/180], Loss: 0.1858\n",
      "Epoch [53/180], Loss: 0.1841\n",
      "Epoch [54/180], Loss: 0.1840\n",
      "Epoch [55/180], Loss: 0.1822\n",
      "Epoch [56/180], Loss: 0.1819\n",
      "Epoch [57/180], Loss: 0.1796\n",
      "Epoch [58/180], Loss: 0.1782\n",
      "Epoch [59/180], Loss: 0.1763\n",
      "Epoch [60/180], Loss: 0.1759\n",
      "Epoch [61/180], Loss: 0.1742\n",
      "Epoch [62/180], Loss: 0.1731\n",
      "Epoch [63/180], Loss: 0.1720\n",
      "Epoch [64/180], Loss: 0.1714\n",
      "Epoch [65/180], Loss: 0.1704\n",
      "Epoch [66/180], Loss: 0.1696\n",
      "Epoch [67/180], Loss: 0.1684\n",
      "Epoch [68/180], Loss: 0.1682\n",
      "Epoch [69/180], Loss: 0.1682\n",
      "Epoch [70/180], Loss: 0.1677\n",
      "Epoch [71/180], Loss: 0.1667\n",
      "Epoch [72/180], Loss: 0.1653\n",
      "Epoch [73/180], Loss: 0.1654\n",
      "Epoch [74/180], Loss: 0.1644\n",
      "Epoch [75/180], Loss: 0.1647\n",
      "Epoch [76/180], Loss: 0.1643\n",
      "Epoch [77/180], Loss: 0.1641\n",
      "Epoch [78/180], Loss: 0.1623\n",
      "Epoch [79/180], Loss: 0.1618\n",
      "Epoch [80/180], Loss: 0.1601\n",
      "Epoch [81/180], Loss: 0.1601\n",
      "Epoch [82/180], Loss: 0.1585\n",
      "Epoch [83/180], Loss: 0.1574\n",
      "Epoch [84/180], Loss: 0.1562\n",
      "Epoch [85/180], Loss: 0.1559\n",
      "Epoch [86/180], Loss: 0.1551\n",
      "Epoch [87/180], Loss: 0.1538\n",
      "Epoch [88/180], Loss: 0.1538\n",
      "Epoch [89/180], Loss: 0.1535\n",
      "Epoch [90/180], Loss: 0.1533\n",
      "Epoch [91/180], Loss: 0.1528\n",
      "Epoch [92/180], Loss: 0.1515\n",
      "Epoch [93/180], Loss: 0.1508\n",
      "Epoch [94/180], Loss: 0.1514\n",
      "Epoch [95/180], Loss: 0.1513\n",
      "Epoch [96/180], Loss: 0.1527\n",
      "Epoch [97/180], Loss: 0.1519\n",
      "Epoch [98/180], Loss: 0.1510\n",
      "Epoch [99/180], Loss: 0.1497\n",
      "Epoch [100/180], Loss: 0.1480\n",
      "Epoch [101/180], Loss: 0.1450\n",
      "Epoch [102/180], Loss: 0.1444\n",
      "Epoch [103/180], Loss: 0.1431\n",
      "Epoch [104/180], Loss: 0.1428\n",
      "Epoch [105/180], Loss: 0.1415\n",
      "Epoch [106/180], Loss: 0.1412\n",
      "Epoch [107/180], Loss: 0.1404\n",
      "Epoch [108/180], Loss: 0.1399\n",
      "Epoch [109/180], Loss: 0.1390\n",
      "Epoch [110/180], Loss: 0.1391\n",
      "Epoch [111/180], Loss: 0.1389\n",
      "Epoch [112/180], Loss: 0.1379\n",
      "Epoch [113/180], Loss: 0.1379\n",
      "Epoch [114/180], Loss: 0.1367\n",
      "Epoch [115/180], Loss: 0.1358\n",
      "Epoch [116/180], Loss: 0.1355\n",
      "Epoch [117/180], Loss: 0.1355\n",
      "Epoch [118/180], Loss: 0.1345\n",
      "Epoch [119/180], Loss: 0.1334\n",
      "Epoch [120/180], Loss: 0.1331\n",
      "Epoch [121/180], Loss: 0.1318\n",
      "Epoch [122/180], Loss: 0.1310\n",
      "Epoch [123/180], Loss: 0.1302\n",
      "Epoch [124/180], Loss: 0.1298\n",
      "Epoch [125/180], Loss: 0.1292\n",
      "Epoch [126/180], Loss: 0.1287\n",
      "Epoch [127/180], Loss: 0.1277\n",
      "Epoch [128/180], Loss: 0.1283\n",
      "Epoch [129/180], Loss: 0.1280\n",
      "Epoch [130/180], Loss: 0.1279\n",
      "Epoch [131/180], Loss: 0.1272\n",
      "Epoch [132/180], Loss: 0.1259\n",
      "Epoch [133/180], Loss: 0.1256\n",
      "Epoch [134/180], Loss: 0.1254\n",
      "Epoch [135/180], Loss: 0.1243\n",
      "Epoch [136/180], Loss: 0.1237\n",
      "Epoch [137/180], Loss: 0.1227\n",
      "Epoch [138/180], Loss: 0.1218\n",
      "Epoch [139/180], Loss: 0.1210\n",
      "Epoch [140/180], Loss: 0.1216\n",
      "Epoch [141/180], Loss: 0.1199\n",
      "Epoch [142/180], Loss: 0.1198\n",
      "Epoch [143/180], Loss: 0.1185\n",
      "Epoch [144/180], Loss: 0.1176\n",
      "Epoch [145/180], Loss: 0.1172\n",
      "Epoch [146/180], Loss: 0.1162\n",
      "Epoch [147/180], Loss: 0.1159\n",
      "Epoch [148/180], Loss: 0.1158\n",
      "Epoch [149/180], Loss: 0.1150\n",
      "Epoch [150/180], Loss: 0.1149\n",
      "Epoch [151/180], Loss: 0.1141\n",
      "Epoch [152/180], Loss: 0.1134\n",
      "Epoch [153/180], Loss: 0.1127\n",
      "Epoch [154/180], Loss: 0.1124\n",
      "Epoch [155/180], Loss: 0.1118\n",
      "Epoch [156/180], Loss: 0.1123\n",
      "Epoch [157/180], Loss: 0.1132\n",
      "Epoch [158/180], Loss: 0.1115\n",
      "Epoch [159/180], Loss: 0.1121\n",
      "Epoch [160/180], Loss: 0.1103\n",
      "Epoch [161/180], Loss: 0.1119\n",
      "Epoch [162/180], Loss: 0.1100\n",
      "Epoch [163/180], Loss: 0.1098\n",
      "Epoch [164/180], Loss: 0.1091\n",
      "Epoch [165/180], Loss: 0.1094\n",
      "Epoch [166/180], Loss: 0.1082\n",
      "Epoch [167/180], Loss: 0.1071\n",
      "Epoch [168/180], Loss: 0.1076\n",
      "Epoch [169/180], Loss: 0.1072\n",
      "Epoch [170/180], Loss: 0.1073\n",
      "Epoch [171/180], Loss: 0.1075\n",
      "Epoch [172/180], Loss: 0.1062\n",
      "Epoch [173/180], Loss: 0.1063\n",
      "Epoch [174/180], Loss: 0.1057\n",
      "Epoch [175/180], Loss: 0.1046\n",
      "Epoch [176/180], Loss: 0.1042\n",
      "Epoch [177/180], Loss: 0.1033\n",
      "Epoch [178/180], Loss: 0.1032\n",
      "Epoch [179/180], Loss: 0.1029\n",
      "Epoch [180/180], Loss: 0.1019\n",
      "AD has finished test evaluation with a test loss of 0.14933072496205568.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "model = AutoDecoder(latent_dim)\n",
    "trainer = BasicTrainer(model=model, dl=train_dl, latent_dim=model.latent_dim, device=device)\n",
    "trainer.train(num_epochs=180)\n",
    "\n",
    "# Evaluation on the test dataset\n",
    "latents = torch.nn.Parameter(torch.randn(len(test_dl.dataset), model.latent_dim).to(device))\n",
    "opt = torch.optim.Adam([latents], lr=3e-3)\n",
    "test_loss = evaluate_model(model=model, test_dl=test_dl, opt=opt, latents=latents, epochs=1000, device=device)\n",
    "print(f\"AD has finished test evaluation with a test loss of {test_loss}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aeR9d4gNdSf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Vectors Shape: torch.Size([5, 256])\n",
      "Random Vectors Shape: torch.Size([5, 256])\n",
      "Sampled Images Shape: torch.Size([5, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 5 indices from the test dataset\n",
    "import random\n",
    "import utils\n",
    "random.seed(6)\n",
    "sampled_indices = random.sample(range(len(latents)), 5)\n",
    "\n",
    "# Extract the corresponding vectors (input data) and their labels\n",
    "sampled_latents = [latents[i] for i in sampled_indices]  # Only selecting input data, not labels\n",
    "\n",
    "# Convert to a single tensor (optional)\n",
    "sampled_latents_tensor = torch.stack(sampled_latents)\n",
    "random_latents_tensor = torch.randn_like(sampled_latents_tensor)\n",
    "\n",
    "print(\"Sampled Vectors Shape:\", sampled_latents_tensor.shape)  # Should be (5, *) depending on your data shape\n",
    "print(\"Random Vectors Shape:\", random_latents_tensor.shape)  # Should be (5, *) depending on your data shape\n",
    "\n",
    "sampled_test_images = model(sampled_latents_tensor).view(-1, 1, 28, 28)\n",
    "random_test_images = model(random_latents_tensor).view(-1, 1, 28, 28)\n",
    "\n",
    "print(\"Sampled Images Shape:\", sampled_test_images.shape)  # Should be (5, *) depending on your data shape\n",
    "utils.save_images(sampled_test_images, \"sampled_test_images.png\")\n",
    "utils.save_images(random_test_images, \"random_test_images.png\")\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(sampled_test_images.detach().cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_tsne\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# def plot_tsne(dataset, latents, file_name, plot_title=\"t-SNE Plot\"):\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plot_tsne(\u001b[43mtest_ds\u001b[49m, latents, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjust a file\u001b[39m\u001b[38;5;124m\"\u001b[39m, plot_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt-SNE Plot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_ds' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import plot_tsne\n",
    "# def plot_tsne(dataset, latents, file_name, plot_title=\"t-SNE Plot\"):\n",
    "plot_tsne(test_ds, latents, \"just a file\", plot_title=\"t-SNE Plot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMxcc5B-c1um"
   },
   "source": [
    "VAD Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataloaders\n",
    "from evaluate import reconstruction_loss, evaluate_model\n",
    "from VariationalAutoDecoder import VariationalAutoDecoder\n",
    "from trainer import VADTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = create_dataloaders(data_path='', batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "model = VariationalAutoDecoder(latent_dim, device=device)\n",
    "\n",
    "trainer = VADTrainer(model=model, dl=train_dl, latent_dim=model.latent_dim, device=device)\n",
    "\n",
    "trainer.train(num_epochs=555)\n",
    "\n",
    "\n",
    "# Evaluation on the test dataset\n",
    "# latents = torch.nn.Parameter(torch.randn(len(test_dl.dataset), model.latent_dim).to(device))\n",
    "# opt = torch.optim.Adam([latents], lr=3e-3)\n",
    "# test_loss = evaluate_model(model=model, test_dl=test_dl, opt=opt, latents=latents, epochs=1000, device=device)\n",
    "# print(f\"AD has finished test evaluation with a test loss of {test_loss}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test dataset\n",
    "# latents = torch.randn((len(test_dl.dataset), model.latent_dim)).to(device)\n",
    "\n",
    "mu_test = torch.randn(len(test_dl.dataset), latent_dim, device=device, requires_grad=True)\n",
    "sigma_test = torch.randn(len(test_dl.dataset), latent_dim, device=device, requires_grad=True)\n",
    "test_latents = torch.nn.parameter.Parameter(torch.stack([mu_test, sigma_test], dim=1)).to(device)\n",
    "\n",
    "opt = torch.optim.Adam([test_latents], lr=3e-3)\n",
    "test_loss = evaluate_model(model=model, test_dl=test_dl, opt=opt, latents=test_latents, epochs=300, device=device)\n",
    "print(f\"AD has finished test evaluation with a test loss of {test_loss}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 5 indices from the test dataset\n",
    "import random\n",
    "import utils\n",
    "random.seed(6)\n",
    "sampled_indices = random.sample(range(len(test_latents)), 5)\n",
    "\n",
    "# Extract the corresponding vectors (input data) and their labels\n",
    "sampled_latents = [test_latents[i] for i in sampled_indices]  # Only selecting input data, not labels\n",
    "\n",
    "# Convert to a single tensor (optional)\n",
    "sampled_latents_tensor = torch.stack(sampled_latents)\n",
    "random_latents_tensor = torch.randn_like(sampled_latents_tensor)\n",
    "\n",
    "print(\"Sampled Vectors Shape:\", sampled_latents_tensor.shape)  # Should be (5, *) depending on your data shape\n",
    "print(\"Random Vectors Shape:\", random_latents_tensor.shape)  # Should be (5, *) depending on your data shape\n",
    "\n",
    "sampled_test_images = model(sampled_latents_tensor).view(-1, 1, 28, 28)\n",
    "random_test_images = model(random_latents_tensor).view(-1, 1, 28, 28)\n",
    "\n",
    "print(\"Sampled Images Shape:\", sampled_test_images.shape)  # Should be (5, *) depending on your data shape\n",
    "utils.save_images(sampled_test_images, \"sampled_test_images.png\")\n",
    "utils.save_images(random_test_images, \"random_test_images.png\")\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(sampled_test_images.detach().cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_tsne\n",
    "\n",
    "result_test_latents = model.reparameterization_trick(test_latents)\n",
    "utils.plot_tsne(test_ds, result_test_latents, f\"tsne_test\")\n",
    "# def plot_tsne(dataset, latents, file_name, plot_title=\"t-SNE Plot\"):\n",
    "plot_tsne(test_ds, result_test_latents, \"just a file variational\", plot_title=\"t-SNE Plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
