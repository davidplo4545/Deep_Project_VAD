{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GmteAhQy9OOM"
   },
   "outputs": [],
   "source": [
    "from utils import create_dataloaders\n",
    "from evaluate import reconstruction_loss, evaluate_model\n",
    "from AutoDecoder import AutoDecoder\n",
    "from trainer import BasicTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_Voa3N89E6w"
   },
   "source": [
    "1.3 Auto Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RTnTKhdwc0-3"
   },
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = create_dataloaders(data_path='', batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQ7uajDHc-7A",
    "outputId": "b6308dfc-9904-4dcb-d932-77382aca3e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/500], Loss: 0.4807\n",
      "Epoch [2/500], Loss: 0.3830\n",
      "Epoch [3/500], Loss: 0.2839\n",
      "Epoch [4/500], Loss: 0.2319\n",
      "Epoch [5/500], Loss: 0.2245\n",
      "Epoch [6/500], Loss: 0.2144\n",
      "Epoch [7/500], Loss: 0.1947\n",
      "Epoch [8/500], Loss: 0.1877\n",
      "Epoch [9/500], Loss: 0.1804\n",
      "Epoch [10/500], Loss: 0.1762\n",
      "Epoch [11/500], Loss: 0.1787\n",
      "Epoch [12/500], Loss: 0.1729\n",
      "Epoch [13/500], Loss: 0.1689\n",
      "Epoch [14/500], Loss: 0.1638\n",
      "Epoch [15/500], Loss: 0.1571\n",
      "Epoch [16/500], Loss: 0.1540\n",
      "Epoch [17/500], Loss: 0.1509\n",
      "Epoch [18/500], Loss: 0.1490\n",
      "Epoch [19/500], Loss: 0.1491\n",
      "Epoch [20/500], Loss: 0.1504\n",
      "Epoch [21/500], Loss: 0.1483\n",
      "Epoch [22/500], Loss: 0.1457\n",
      "Epoch [23/500], Loss: 0.1427\n",
      "Epoch [24/500], Loss: 0.1377\n",
      "Epoch [25/500], Loss: 0.1383\n",
      "Epoch [26/500], Loss: 0.1372\n",
      "Epoch [27/500], Loss: 0.1341\n",
      "Epoch [28/500], Loss: 0.1339\n",
      "Epoch [29/500], Loss: 0.1311\n",
      "Epoch [30/500], Loss: 0.1258\n",
      "Epoch [31/500], Loss: 0.1238\n",
      "Epoch [32/500], Loss: 0.1201\n",
      "Epoch [33/500], Loss: 0.1222\n",
      "Epoch [34/500], Loss: 0.1212\n",
      "Epoch [35/500], Loss: 0.1200\n",
      "Epoch [36/500], Loss: 0.1164\n",
      "Epoch [37/500], Loss: 0.1126\n",
      "Epoch [38/500], Loss: 0.1118\n",
      "Epoch [39/500], Loss: 0.1119\n",
      "Epoch [40/500], Loss: 0.1124\n",
      "Epoch [41/500], Loss: 0.1128\n",
      "Epoch [42/500], Loss: 0.1095\n",
      "Epoch [43/500], Loss: 0.1049\n",
      "Epoch [44/500], Loss: 0.1024\n",
      "Epoch [45/500], Loss: 0.1052\n",
      "Epoch [46/500], Loss: 0.1072\n",
      "Epoch [47/500], Loss: 0.1054\n",
      "Epoch [48/500], Loss: 0.0997\n",
      "Epoch [49/500], Loss: 0.0923\n",
      "Epoch [50/500], Loss: 0.0905\n",
      "Epoch [51/500], Loss: 0.0930\n",
      "Epoch [52/500], Loss: 0.0947\n",
      "Epoch [53/500], Loss: 0.0957\n",
      "Epoch [54/500], Loss: 0.0972\n",
      "Epoch [55/500], Loss: 0.0955\n",
      "Epoch [56/500], Loss: 0.0893\n",
      "Epoch [57/500], Loss: 0.0860\n",
      "Epoch [58/500], Loss: 0.0860\n",
      "Epoch [59/500], Loss: 0.0885\n",
      "Epoch [60/500], Loss: 0.0872\n",
      "Epoch [61/500], Loss: 0.0847\n",
      "Epoch [62/500], Loss: 0.0831\n",
      "Epoch [63/500], Loss: 0.0837\n",
      "Epoch [64/500], Loss: 0.0839\n",
      "Epoch [65/500], Loss: 0.0802\n",
      "Epoch [66/500], Loss: 0.0804\n",
      "Epoch [67/500], Loss: 0.0828\n",
      "Epoch [68/500], Loss: 0.0853\n",
      "Epoch [69/500], Loss: 0.0864\n",
      "Epoch [70/500], Loss: 0.0816\n",
      "Epoch [71/500], Loss: 0.0775\n",
      "Epoch [72/500], Loss: 0.0751\n",
      "Epoch [73/500], Loss: 0.0759\n",
      "Epoch [74/500], Loss: 0.0799\n",
      "Epoch [75/500], Loss: 0.0800\n",
      "Epoch [76/500], Loss: 0.0798\n",
      "Epoch [77/500], Loss: 0.0766\n",
      "Epoch [78/500], Loss: 0.0715\n",
      "Epoch [79/500], Loss: 0.0721\n",
      "Epoch [80/500], Loss: 0.0742\n",
      "Epoch [81/500], Loss: 0.0750\n",
      "Epoch [82/500], Loss: 0.0787\n",
      "Epoch [83/500], Loss: 0.0766\n",
      "Epoch [84/500], Loss: 0.0701\n",
      "Epoch [85/500], Loss: 0.0687\n",
      "Epoch [86/500], Loss: 0.0718\n",
      "Epoch [87/500], Loss: 0.0725\n",
      "Epoch [88/500], Loss: 0.0745\n",
      "Epoch [89/500], Loss: 0.0711\n",
      "Epoch [90/500], Loss: 0.0653\n",
      "Epoch [91/500], Loss: 0.0653\n",
      "Epoch [92/500], Loss: 0.0701\n",
      "Epoch [93/500], Loss: 0.0709\n",
      "Epoch [94/500], Loss: 0.0714\n",
      "Epoch [95/500], Loss: 0.0680\n",
      "Epoch [96/500], Loss: 0.0620\n",
      "Epoch [97/500], Loss: 0.0639\n",
      "Epoch [98/500], Loss: 0.0687\n",
      "Epoch [99/500], Loss: 0.0694\n",
      "Epoch [100/500], Loss: 0.0705\n",
      "Epoch [101/500], Loss: 0.0698\n",
      "Epoch [102/500], Loss: 0.0645\n",
      "Epoch [103/500], Loss: 0.0628\n",
      "Epoch [104/500], Loss: 0.0634\n",
      "Epoch [105/500], Loss: 0.0648\n",
      "Epoch [106/500], Loss: 0.0629\n",
      "Epoch [107/500], Loss: 0.0633\n",
      "Epoch [108/500], Loss: 0.0608\n",
      "Epoch [109/500], Loss: 0.0604\n",
      "Epoch [110/500], Loss: 0.0634\n",
      "Epoch [111/500], Loss: 0.0652\n",
      "Epoch [112/500], Loss: 0.0663\n",
      "Epoch [113/500], Loss: 0.0661\n",
      "Epoch [114/500], Loss: 0.0614\n",
      "Epoch [115/500], Loss: 0.0586\n",
      "Epoch [116/500], Loss: 0.0605\n",
      "Epoch [117/500], Loss: 0.0655\n",
      "Epoch [118/500], Loss: 0.0660\n",
      "Epoch [119/500], Loss: 0.0625\n",
      "Epoch [120/500], Loss: 0.0597\n",
      "Epoch [121/500], Loss: 0.0587\n",
      "Epoch [122/500], Loss: 0.0591\n",
      "Epoch [123/500], Loss: 0.0635\n",
      "Epoch [124/500], Loss: 0.0638\n",
      "Epoch [125/500], Loss: 0.0600\n",
      "Epoch [126/500], Loss: 0.0590\n",
      "Epoch [127/500], Loss: 0.0608\n",
      "Epoch [128/500], Loss: 0.0584\n",
      "Epoch [129/500], Loss: 0.0560\n",
      "Epoch [130/500], Loss: 0.0572\n",
      "Epoch [131/500], Loss: 0.0571\n",
      "Epoch [132/500], Loss: 0.0559\n",
      "Epoch [133/500], Loss: 0.0578\n",
      "Epoch [134/500], Loss: 0.0565\n",
      "Epoch [135/500], Loss: 0.0548\n",
      "Epoch [136/500], Loss: 0.0548\n",
      "Epoch [137/500], Loss: 0.0559\n",
      "Epoch [138/500], Loss: 0.0584\n",
      "Epoch [139/500], Loss: 0.0624\n",
      "Epoch [140/500], Loss: 0.0589\n",
      "Epoch [141/500], Loss: 0.0557\n",
      "Epoch [142/500], Loss: 0.0521\n",
      "Epoch [143/500], Loss: 0.0528\n",
      "Epoch [144/500], Loss: 0.0572\n",
      "Epoch [145/500], Loss: 0.0604\n",
      "Epoch [146/500], Loss: 0.0607\n",
      "Epoch [147/500], Loss: 0.0581\n",
      "Epoch [148/500], Loss: 0.0505\n",
      "Epoch [149/500], Loss: 0.0504\n",
      "Epoch [150/500], Loss: 0.0534\n",
      "Epoch [151/500], Loss: 0.0550\n",
      "Epoch [152/500], Loss: 0.0540\n",
      "Epoch [153/500], Loss: 0.0530\n",
      "Epoch [154/500], Loss: 0.0516\n",
      "Epoch [155/500], Loss: 0.0509\n",
      "Epoch [156/500], Loss: 0.0522\n",
      "Epoch [157/500], Loss: 0.0540\n",
      "Epoch [158/500], Loss: 0.0544\n",
      "Epoch [159/500], Loss: 0.0516\n",
      "Epoch [160/500], Loss: 0.0516\n",
      "Epoch [161/500], Loss: 0.0491\n",
      "Epoch [162/500], Loss: 0.0486\n",
      "Epoch [163/500], Loss: 0.0532\n",
      "Epoch [164/500], Loss: 0.0545\n",
      "Epoch [165/500], Loss: 0.0514\n",
      "Epoch [166/500], Loss: 0.0498\n",
      "Epoch [167/500], Loss: 0.0472\n",
      "Epoch [168/500], Loss: 0.0475\n",
      "Epoch [169/500], Loss: 0.0497\n",
      "Epoch [170/500], Loss: 0.0504\n",
      "Epoch [171/500], Loss: 0.0473\n",
      "Epoch [172/500], Loss: 0.0486\n",
      "Epoch [173/500], Loss: 0.0491\n",
      "Epoch [174/500], Loss: 0.0477\n",
      "Epoch [175/500], Loss: 0.0472\n",
      "Epoch [176/500], Loss: 0.0469\n",
      "Epoch [177/500], Loss: 0.0463\n",
      "Epoch [178/500], Loss: 0.0476\n",
      "Epoch [179/500], Loss: 0.0469\n",
      "Epoch [180/500], Loss: 0.0486\n",
      "Epoch [181/500], Loss: 0.0446\n",
      "Epoch [182/500], Loss: 0.0458\n",
      "Epoch [183/500], Loss: 0.0447\n",
      "Epoch [184/500], Loss: 0.0426\n",
      "Epoch [185/500], Loss: 0.0423\n",
      "Epoch [186/500], Loss: 0.0443\n",
      "Epoch [187/500], Loss: 0.0463\n",
      "Epoch [188/500], Loss: 0.0466\n",
      "Epoch [189/500], Loss: 0.0439\n",
      "Epoch [190/500], Loss: 0.0429\n",
      "Epoch [191/500], Loss: 0.0450\n",
      "Epoch [192/500], Loss: 0.0473\n",
      "Epoch [193/500], Loss: 0.0455\n",
      "Epoch [194/500], Loss: 0.0427\n",
      "Epoch [195/500], Loss: 0.0394\n",
      "Epoch [196/500], Loss: 0.0408\n",
      "Epoch [197/500], Loss: 0.0405\n",
      "Epoch [198/500], Loss: 0.0415\n",
      "Epoch [199/500], Loss: 0.0436\n",
      "Epoch [200/500], Loss: 0.0437\n",
      "Epoch [201/500], Loss: 0.0403\n",
      "Epoch [202/500], Loss: 0.0380\n",
      "Epoch [203/500], Loss: 0.0397\n",
      "Epoch [204/500], Loss: 0.0445\n",
      "Epoch [205/500], Loss: 0.0427\n",
      "Epoch [206/500], Loss: 0.0414\n",
      "Epoch [207/500], Loss: 0.0405\n",
      "Epoch [208/500], Loss: 0.0374\n",
      "Epoch [209/500], Loss: 0.0383\n",
      "Epoch [210/500], Loss: 0.0434\n",
      "Epoch [211/500], Loss: 0.0426\n",
      "Epoch [212/500], Loss: 0.0404\n",
      "Epoch [213/500], Loss: 0.0409\n",
      "Epoch [214/500], Loss: 0.0388\n",
      "Epoch [215/500], Loss: 0.0391\n",
      "Epoch [216/500], Loss: 0.0393\n",
      "Epoch [217/500], Loss: 0.0412\n",
      "Epoch [218/500], Loss: 0.0400\n",
      "Epoch [219/500], Loss: 0.0407\n",
      "Epoch [220/500], Loss: 0.0422\n",
      "Epoch [221/500], Loss: 0.0381\n",
      "Epoch [222/500], Loss: 0.0368\n",
      "Epoch [223/500], Loss: 0.0407\n",
      "Epoch [224/500], Loss: 0.0424\n",
      "Epoch [225/500], Loss: 0.0398\n",
      "Epoch [226/500], Loss: 0.0381\n",
      "Epoch [227/500], Loss: 0.0376\n",
      "Epoch [228/500], Loss: 0.0360\n",
      "Epoch [229/500], Loss: 0.0363\n",
      "Epoch [230/500], Loss: 0.0398\n",
      "Epoch [231/500], Loss: 0.0405\n",
      "Epoch [232/500], Loss: 0.0375\n",
      "Epoch [233/500], Loss: 0.0379\n",
      "Epoch [234/500], Loss: 0.0383\n",
      "Epoch [235/500], Loss: 0.0354\n",
      "Epoch [236/500], Loss: 0.0375\n",
      "Epoch [237/500], Loss: 0.0393\n",
      "Epoch [238/500], Loss: 0.0368\n",
      "Epoch [239/500], Loss: 0.0371\n",
      "Epoch [240/500], Loss: 0.0386\n",
      "Epoch [241/500], Loss: 0.0366\n",
      "Epoch [242/500], Loss: 0.0356\n",
      "Epoch [243/500], Loss: 0.0372\n",
      "Epoch [244/500], Loss: 0.0372\n",
      "Epoch [245/500], Loss: 0.0375\n",
      "Epoch [246/500], Loss: 0.0377\n",
      "Epoch [247/500], Loss: 0.0381\n",
      "Epoch [248/500], Loss: 0.0370\n",
      "Epoch [249/500], Loss: 0.0355\n",
      "Epoch [250/500], Loss: 0.0357\n",
      "Epoch [251/500], Loss: 0.0364\n",
      "Epoch [252/500], Loss: 0.0359\n",
      "Epoch [253/500], Loss: 0.0377\n",
      "Epoch [254/500], Loss: 0.0388\n",
      "Epoch [255/500], Loss: 0.0350\n",
      "Epoch [256/500], Loss: 0.0342\n",
      "Epoch [257/500], Loss: 0.0352\n",
      "Epoch [258/500], Loss: 0.0348\n",
      "Epoch [259/500], Loss: 0.0354\n",
      "Epoch [260/500], Loss: 0.0369\n",
      "Epoch [261/500], Loss: 0.0370\n",
      "Epoch [262/500], Loss: 0.0336\n",
      "Epoch [263/500], Loss: 0.0335\n",
      "Epoch [264/500], Loss: 0.0365\n",
      "Epoch [265/500], Loss: 0.0370\n",
      "Epoch [266/500], Loss: 0.0363\n",
      "Epoch [267/500], Loss: 0.0362\n",
      "Epoch [268/500], Loss: 0.0358\n",
      "Epoch [269/500], Loss: 0.0330\n",
      "Epoch [270/500], Loss: 0.0353\n",
      "Epoch [271/500], Loss: 0.0369\n",
      "Epoch [272/500], Loss: 0.0378\n",
      "Epoch [273/500], Loss: 0.0344\n",
      "Epoch [274/500], Loss: 0.0338\n",
      "Epoch [275/500], Loss: 0.0330\n",
      "Epoch [276/500], Loss: 0.0317\n",
      "Epoch [277/500], Loss: 0.0340\n",
      "Epoch [278/500], Loss: 0.0370\n",
      "Epoch [279/500], Loss: 0.0354\n",
      "Epoch [280/500], Loss: 0.0347\n",
      "Epoch [281/500], Loss: 0.0360\n",
      "Epoch [282/500], Loss: 0.0322\n",
      "Epoch [283/500], Loss: 0.0306\n",
      "Epoch [284/500], Loss: 0.0325\n",
      "Epoch [285/500], Loss: 0.0360\n",
      "Epoch [286/500], Loss: 0.0357\n",
      "Epoch [287/500], Loss: 0.0345\n",
      "Epoch [288/500], Loss: 0.0339\n",
      "Epoch [289/500], Loss: 0.0323\n",
      "Epoch [290/500], Loss: 0.0313\n",
      "Epoch [291/500], Loss: 0.0331\n",
      "Epoch [292/500], Loss: 0.0352\n",
      "Epoch [293/500], Loss: 0.0328\n",
      "Epoch [294/500], Loss: 0.0331\n",
      "Epoch [295/500], Loss: 0.0340\n",
      "Epoch [296/500], Loss: 0.0310\n",
      "Epoch [297/500], Loss: 0.0311\n",
      "Epoch [298/500], Loss: 0.0342\n",
      "Epoch [299/500], Loss: 0.0363\n",
      "Epoch [300/500], Loss: 0.0345\n",
      "Epoch [301/500], Loss: 0.0339\n",
      "Epoch [302/500], Loss: 0.0329\n",
      "Epoch [303/500], Loss: 0.0313\n",
      "Epoch [304/500], Loss: 0.0323\n",
      "Epoch [305/500], Loss: 0.0340\n",
      "Epoch [306/500], Loss: 0.0334\n",
      "Epoch [307/500], Loss: 0.0327\n",
      "Epoch [308/500], Loss: 0.0360\n",
      "Epoch [309/500], Loss: 0.0361\n",
      "Epoch [310/500], Loss: 0.0325\n",
      "Epoch [311/500], Loss: 0.0330\n",
      "Epoch [312/500], Loss: 0.0332\n",
      "Epoch [313/500], Loss: 0.0323\n",
      "Epoch [314/500], Loss: 0.0327\n",
      "Epoch [315/500], Loss: 0.0334\n",
      "Epoch [316/500], Loss: 0.0303\n",
      "Epoch [317/500], Loss: 0.0302\n",
      "Epoch [318/500], Loss: 0.0317\n",
      "Epoch [319/500], Loss: 0.0341\n",
      "Epoch [320/500], Loss: 0.0319\n",
      "Epoch [321/500], Loss: 0.0320\n",
      "Epoch [322/500], Loss: 0.0333\n",
      "Epoch [323/500], Loss: 0.0322\n",
      "Epoch [324/500], Loss: 0.0308\n",
      "Epoch [325/500], Loss: 0.0324\n",
      "Epoch [326/500], Loss: 0.0326\n",
      "Epoch [327/500], Loss: 0.0315\n",
      "Epoch [328/500], Loss: 0.0323\n",
      "Epoch [329/500], Loss: 0.0330\n",
      "Epoch [330/500], Loss: 0.0309\n",
      "Epoch [331/500], Loss: 0.0309\n",
      "Epoch [332/500], Loss: 0.0320\n",
      "Epoch [333/500], Loss: 0.0317\n",
      "Epoch [334/500], Loss: 0.0308\n",
      "Epoch [335/500], Loss: 0.0318\n",
      "Epoch [336/500], Loss: 0.0324\n",
      "Epoch [337/500], Loss: 0.0296\n",
      "Epoch [338/500], Loss: 0.0305\n",
      "Epoch [339/500], Loss: 0.0331\n",
      "Epoch [340/500], Loss: 0.0328\n",
      "Epoch [341/500], Loss: 0.0316\n",
      "Epoch [342/500], Loss: 0.0316\n",
      "Epoch [343/500], Loss: 0.0307\n",
      "Epoch [344/500], Loss: 0.0301\n",
      "Epoch [345/500], Loss: 0.0309\n",
      "Epoch [346/500], Loss: 0.0324\n",
      "Epoch [347/500], Loss: 0.0312\n",
      "Epoch [348/500], Loss: 0.0306\n",
      "Epoch [349/500], Loss: 0.0311\n",
      "Epoch [350/500], Loss: 0.0315\n",
      "Epoch [351/500], Loss: 0.0310\n",
      "Epoch [352/500], Loss: 0.0310\n",
      "Epoch [353/500], Loss: 0.0314\n",
      "Epoch [354/500], Loss: 0.0295\n",
      "Epoch [355/500], Loss: 0.0295\n",
      "Epoch [356/500], Loss: 0.0316\n",
      "Epoch [357/500], Loss: 0.0300\n",
      "Epoch [358/500], Loss: 0.0305\n",
      "Epoch [359/500], Loss: 0.0312\n",
      "Epoch [360/500], Loss: 0.0328\n",
      "Epoch [361/500], Loss: 0.0324\n",
      "Epoch [362/500], Loss: 0.0311\n",
      "Epoch [363/500], Loss: 0.0300\n",
      "Epoch [364/500], Loss: 0.0295\n",
      "Epoch [365/500], Loss: 0.0287\n",
      "Epoch [366/500], Loss: 0.0309\n",
      "Epoch [367/500], Loss: 0.0323\n",
      "Epoch [368/500], Loss: 0.0308\n",
      "Epoch [369/500], Loss: 0.0300\n",
      "Epoch [370/500], Loss: 0.0304\n",
      "Epoch [371/500], Loss: 0.0282\n",
      "Epoch [372/500], Loss: 0.0277\n",
      "Epoch [373/500], Loss: 0.0299\n",
      "Epoch [374/500], Loss: 0.0302\n",
      "Epoch [375/500], Loss: 0.0307\n",
      "Epoch [376/500], Loss: 0.0318\n",
      "Epoch [377/500], Loss: 0.0324\n",
      "Epoch [378/500], Loss: 0.0282\n",
      "Epoch [379/500], Loss: 0.0272\n",
      "Epoch [380/500], Loss: 0.0293\n",
      "Epoch [381/500], Loss: 0.0308\n",
      "Epoch [382/500], Loss: 0.0310\n",
      "Epoch [383/500], Loss: 0.0320\n",
      "Epoch [384/500], Loss: 0.0303\n",
      "Epoch [385/500], Loss: 0.0285\n",
      "Epoch [386/500], Loss: 0.0290\n",
      "Epoch [387/500], Loss: 0.0299\n",
      "Epoch [388/500], Loss: 0.0314\n",
      "Epoch [389/500], Loss: 0.0309\n",
      "Epoch [390/500], Loss: 0.0293\n",
      "Epoch [391/500], Loss: 0.0286\n",
      "Epoch [392/500], Loss: 0.0275\n",
      "Epoch [393/500], Loss: 0.0287\n",
      "Epoch [394/500], Loss: 0.0324\n",
      "Epoch [395/500], Loss: 0.0284\n",
      "Epoch [396/500], Loss: 0.0274\n",
      "Epoch [397/500], Loss: 0.0293\n",
      "Epoch [398/500], Loss: 0.0298\n",
      "Epoch [399/500], Loss: 0.0286\n",
      "Epoch [400/500], Loss: 0.0287\n",
      "Epoch [401/500], Loss: 0.0285\n",
      "Epoch [402/500], Loss: 0.0287\n",
      "Epoch [403/500], Loss: 0.0281\n",
      "Epoch [404/500], Loss: 0.0291\n",
      "Epoch [405/500], Loss: 0.0308\n",
      "Epoch [406/500], Loss: 0.0284\n",
      "Epoch [407/500], Loss: 0.0272\n",
      "Epoch [408/500], Loss: 0.0290\n",
      "Epoch [409/500], Loss: 0.0289\n",
      "Epoch [410/500], Loss: 0.0298\n",
      "Epoch [411/500], Loss: 0.0299\n",
      "Epoch [412/500], Loss: 0.0278\n",
      "Epoch [413/500], Loss: 0.0273\n",
      "Epoch [414/500], Loss: 0.0285\n",
      "Epoch [415/500], Loss: 0.0295\n",
      "Epoch [416/500], Loss: 0.0320\n",
      "Epoch [417/500], Loss: 0.0306\n",
      "Epoch [418/500], Loss: 0.0287\n",
      "Epoch [419/500], Loss: 0.0290\n",
      "Epoch [420/500], Loss: 0.0296\n",
      "Epoch [421/500], Loss: 0.0283\n",
      "Epoch [422/500], Loss: 0.0297\n",
      "Epoch [423/500], Loss: 0.0290\n",
      "Epoch [424/500], Loss: 0.0296\n",
      "Epoch [425/500], Loss: 0.0302\n",
      "Epoch [426/500], Loss: 0.0289\n",
      "Epoch [427/500], Loss: 0.0286\n",
      "Epoch [428/500], Loss: 0.0286\n",
      "Epoch [429/500], Loss: 0.0304\n",
      "Epoch [430/500], Loss: 0.0284\n",
      "Epoch [431/500], Loss: 0.0263\n",
      "Epoch [432/500], Loss: 0.0274\n",
      "Epoch [433/500], Loss: 0.0288\n",
      "Epoch [434/500], Loss: 0.0282\n",
      "Epoch [435/500], Loss: 0.0282\n",
      "Epoch [436/500], Loss: 0.0277\n",
      "Epoch [437/500], Loss: 0.0278\n",
      "Epoch [438/500], Loss: 0.0277\n",
      "Epoch [439/500], Loss: 0.0288\n",
      "Epoch [440/500], Loss: 0.0280\n",
      "Epoch [441/500], Loss: 0.0262\n",
      "Epoch [442/500], Loss: 0.0272\n",
      "Epoch [443/500], Loss: 0.0302\n",
      "Epoch [444/500], Loss: 0.0298\n",
      "Epoch [445/500], Loss: 0.0293\n",
      "Epoch [446/500], Loss: 0.0286\n",
      "Epoch [447/500], Loss: 0.0268\n",
      "Epoch [448/500], Loss: 0.0253\n",
      "Epoch [449/500], Loss: 0.0277\n",
      "Epoch [450/500], Loss: 0.0306\n",
      "Epoch [451/500], Loss: 0.0287\n",
      "Epoch [452/500], Loss: 0.0268\n",
      "Epoch [453/500], Loss: 0.0273\n",
      "Epoch [454/500], Loss: 0.0251\n",
      "Epoch [455/500], Loss: 0.0257\n",
      "Epoch [456/500], Loss: 0.0283\n",
      "Epoch [457/500], Loss: 0.0292\n",
      "Epoch [458/500], Loss: 0.0271\n",
      "Epoch [459/500], Loss: 0.0264\n",
      "Epoch [460/500], Loss: 0.0281\n",
      "Epoch [461/500], Loss: 0.0277\n",
      "Epoch [462/500], Loss: 0.0266\n",
      "Epoch [463/500], Loss: 0.0278\n",
      "Epoch [464/500], Loss: 0.0301\n",
      "Epoch [465/500], Loss: 0.0289\n",
      "Epoch [466/500], Loss: 0.0273\n",
      "Epoch [467/500], Loss: 0.0274\n",
      "Epoch [468/500], Loss: 0.0264\n",
      "Epoch [469/500], Loss: 0.0256\n",
      "Epoch [470/500], Loss: 0.0260\n",
      "Epoch [471/500], Loss: 0.0286\n",
      "Epoch [472/500], Loss: 0.0273\n",
      "Epoch [473/500], Loss: 0.0272\n",
      "Epoch [474/500], Loss: 0.0279\n",
      "Epoch [475/500], Loss: 0.0271\n",
      "Epoch [476/500], Loss: 0.0261\n",
      "Epoch [477/500], Loss: 0.0277\n",
      "Epoch [478/500], Loss: 0.0276\n",
      "Epoch [479/500], Loss: 0.0260\n",
      "Epoch [480/500], Loss: 0.0273\n",
      "Epoch [481/500], Loss: 0.0299\n",
      "Epoch [482/500], Loss: 0.0291\n",
      "Epoch [483/500], Loss: 0.0262\n",
      "Epoch [484/500], Loss: 0.0254\n",
      "Epoch [485/500], Loss: 0.0271\n",
      "Epoch [486/500], Loss: 0.0272\n",
      "Epoch [487/500], Loss: 0.0274\n",
      "Epoch [488/500], Loss: 0.0276\n",
      "Epoch [489/500], Loss: 0.0261\n",
      "Epoch [490/500], Loss: 0.0239\n",
      "Epoch [491/500], Loss: 0.0258\n",
      "Epoch [492/500], Loss: 0.0286\n",
      "Epoch [493/500], Loss: 0.0277\n",
      "Epoch [494/500], Loss: 0.0281\n",
      "Epoch [495/500], Loss: 0.0290\n",
      "Epoch [496/500], Loss: 0.0255\n",
      "Epoch [497/500], Loss: 0.0239\n",
      "Epoch [498/500], Loss: 0.0246\n",
      "Epoch [499/500], Loss: 0.0271\n",
      "Epoch [500/500], Loss: 0.0271\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "model = AutoDecoder(latent_dim)\n",
    "trainer = BasicTrainer(model=model, dl=train_dl, latent_dim=model.latent_dim, device=device)\n",
    "num_epochs = 500\n",
    "losses = trainer.train(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD has finished train evaluation with a train loss of 0.02666552329901606.\n",
      "AD has finished test evaluation with a test loss of 0.08844717731699347.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the training dataset\n",
    "train_latents = torch.nn.Parameter(torch.randn(len(train_dl.dataset), model.latent_dim).to(device))\n",
    "opt = torch.optim.Adam([train_latents], lr=1e-3)\n",
    "test_loss = evaluate_model(model=model, test_dl=train_dl, opt=opt, latents=train_latents, epochs=1000, device=device)\n",
    "print(f\"AD has finished train evaluation with a train loss of {test_loss}.\")\n",
    "\n",
    "# Evaluation on the test dataset\n",
    "latents = torch.nn.Parameter(torch.randn(len(test_dl.dataset), model.latent_dim).to(device))\n",
    "opt = torch.optim.Adam([latents], lr=1e-3)\n",
    "test_loss = evaluate_model(model=model, test_dl=test_dl, opt=opt, latents=latents, epochs=1000, device=device)\n",
    "print(f\"AD has finished test evaluation with a test loss of {test_loss}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(8, 6))\n",
    "# print(test_loss)\n",
    "plt.plot(epochs, losses, label='Training Loss', color='blue')\n",
    "# plt.plot(epochs,test_loss , label='Test Loss', color='orange')  \n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Training Loss Over Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "# Adding grid\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aeR9d4gNdSf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Vectors Shape: torch.Size([5, 128])\n",
      "Random Vectors Shape: torch.Size([5, 128])\n",
      "Sampled Images Shape: torch.Size([5, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 5 indices from the test dataset\n",
    "import random\n",
    "import utils\n",
    "random.seed(42)\n",
    "sampled_indices = random.sample(range(len(latents)), 5)\n",
    "\n",
    "# Extract the corresponding vectors (input data) and their labels\n",
    "sampled_latents = [latents[i] for i in sampled_indices]  # Only selecting input data, not labels\n",
    "\n",
    "# Convert to a single tensor (optional)\n",
    "sampled_latents_tensor = torch.stack(sampled_latents)\n",
    "random_latents_tensor = torch.randn_like(sampled_latents_tensor)\n",
    "\n",
    "print(\"Sampled Vectors Shape:\", sampled_latents_tensor.shape)  # Should be (5, *) depending on your data shape\n",
    "print(\"Random Vectors Shape:\", random_latents_tensor.shape)  # Should be (5, *) depending on your data shape\n",
    "\n",
    "sampled_test_images = model(sampled_latents_tensor).view(-1, 1, 28, 28)\n",
    "random_test_images = model(random_latents_tensor).view(-1, 1, 28, 28)\n",
    "\n",
    "print(\"Sampled Images Shape:\", sampled_test_images.shape)  # Should be (5, *) depending on your data shape\n",
    "utils.save_images(sampled_test_images, \"AD_results/generated/sampled_test_images.png\")\n",
    "utils.save_images(random_test_images, \"AD_results/generated/random_test_images.png\")\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(sampled_test_images.detach().cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import plot_tsne\n",
    "utils.plot_tsne(test_ds, latents, f\"AD_results/tsne/tsne_test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMxcc5B-c1um"
   },
   "source": [
    "VAD Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataloaders\n",
    "from evaluate import reconstruction_loss, evaluate_model\n",
    "from VariationalAutoDecoder import VariationalAutoDecoder\n",
    "from trainer import VADTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = create_dataloaders(data_path='', batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/350], Loss: 40672.8723\n",
      "Epoch [2/350], Loss: 37561.4243\n",
      "Epoch [3/350], Loss: 35403.7803\n",
      "Epoch [4/350], Loss: 34148.6423\n",
      "Epoch [5/350], Loss: 33152.8794\n",
      "Epoch [6/350], Loss: 32544.6323\n",
      "Epoch [7/350], Loss: 32163.6489\n",
      "Epoch [8/350], Loss: 31757.8673\n",
      "Epoch [9/350], Loss: 31354.0328\n",
      "Epoch [10/350], Loss: 31095.6456\n",
      "Epoch [11/350], Loss: 30722.6014\n",
      "Epoch [12/350], Loss: 30393.0782\n",
      "Epoch [13/350], Loss: 30154.4926\n",
      "Epoch [14/350], Loss: 29803.0768\n",
      "Epoch [15/350], Loss: 29520.1035\n",
      "Epoch [16/350], Loss: 29332.0758\n",
      "Epoch [17/350], Loss: 28949.1434\n",
      "Epoch [18/350], Loss: 28714.1139\n",
      "Epoch [19/350], Loss: 28508.0255\n",
      "Epoch [20/350], Loss: 28252.1968\n",
      "Epoch [21/350], Loss: 28043.5146\n",
      "Epoch [22/350], Loss: 27784.6937\n",
      "Epoch [23/350], Loss: 27517.7429\n",
      "Epoch [24/350], Loss: 27252.7223\n",
      "Epoch [25/350], Loss: 27060.4987\n",
      "Epoch [26/350], Loss: 26856.2653\n",
      "Epoch [27/350], Loss: 26590.8492\n",
      "Epoch [28/350], Loss: 26373.8333\n",
      "Epoch [29/350], Loss: 26173.0114\n",
      "Epoch [30/350], Loss: 25929.3774\n",
      "Epoch [31/350], Loss: 25703.1072\n",
      "Epoch [32/350], Loss: 25479.1134\n",
      "Epoch [33/350], Loss: 25357.8292\n",
      "Epoch [34/350], Loss: 25048.8497\n",
      "Epoch [35/350], Loss: 24957.8358\n",
      "Epoch [36/350], Loss: 24674.3457\n",
      "Epoch [37/350], Loss: 24500.1764\n",
      "Epoch [38/350], Loss: 24366.1178\n",
      "Epoch [39/350], Loss: 24154.4822\n",
      "Epoch [40/350], Loss: 23993.3175\n",
      "Epoch [41/350], Loss: 23770.7982\n",
      "Epoch [42/350], Loss: 23671.9663\n",
      "Epoch [43/350], Loss: 23545.5848\n",
      "Epoch [44/350], Loss: 23352.5382\n",
      "Epoch [45/350], Loss: 23172.4943\n",
      "Epoch [46/350], Loss: 22980.8308\n",
      "Epoch [47/350], Loss: 22849.2548\n",
      "Epoch [48/350], Loss: 22628.1882\n",
      "Epoch [49/350], Loss: 22456.2704\n",
      "Epoch [50/350], Loss: 22294.8440\n",
      "Epoch [51/350], Loss: 22202.9373\n",
      "Epoch [52/350], Loss: 21995.5042\n",
      "Epoch [53/350], Loss: 21808.0133\n",
      "Epoch [54/350], Loss: 21651.0454\n",
      "Epoch [55/350], Loss: 21582.1321\n",
      "Epoch [56/350], Loss: 21404.1909\n",
      "Epoch [57/350], Loss: 21290.4696\n",
      "Epoch [58/350], Loss: 21131.6357\n",
      "Epoch [59/350], Loss: 20962.7189\n",
      "Epoch [60/350], Loss: 20838.8074\n",
      "Epoch [61/350], Loss: 20765.3896\n",
      "Epoch [62/350], Loss: 20579.1332\n",
      "Epoch [63/350], Loss: 20427.2529\n",
      "Epoch [64/350], Loss: 20330.7627\n",
      "Epoch [65/350], Loss: 20169.5569\n",
      "Epoch [66/350], Loss: 20034.0459\n",
      "Epoch [67/350], Loss: 19958.4968\n",
      "Epoch [68/350], Loss: 19838.3063\n",
      "Epoch [69/350], Loss: 19655.9165\n",
      "Epoch [70/350], Loss: 19550.7035\n",
      "Epoch [71/350], Loss: 19390.2394\n",
      "Epoch [72/350], Loss: 19324.0142\n",
      "Epoch [73/350], Loss: 19173.0088\n",
      "Epoch [74/350], Loss: 19077.2395\n",
      "Epoch [75/350], Loss: 18873.2290\n",
      "Epoch [76/350], Loss: 18855.2491\n",
      "Epoch [77/350], Loss: 18697.9321\n",
      "Epoch [78/350], Loss: 18562.5385\n",
      "Epoch [79/350], Loss: 18451.9429\n",
      "Epoch [80/350], Loss: 18297.7529\n",
      "Epoch [81/350], Loss: 18219.8479\n",
      "Epoch [82/350], Loss: 18042.9839\n",
      "Epoch [83/350], Loss: 18074.0557\n",
      "Epoch [84/350], Loss: 17945.7120\n",
      "Epoch [85/350], Loss: 17771.9204\n",
      "Epoch [86/350], Loss: 17721.9698\n",
      "Epoch [87/350], Loss: 17544.2262\n",
      "Epoch [88/350], Loss: 17526.1644\n",
      "Epoch [89/350], Loss: 17416.9464\n",
      "Epoch [90/350], Loss: 17272.1274\n",
      "Epoch [91/350], Loss: 17170.2334\n",
      "Epoch [92/350], Loss: 17080.3690\n",
      "Epoch [93/350], Loss: 16980.7257\n",
      "Epoch [94/350], Loss: 16835.7448\n",
      "Epoch [95/350], Loss: 16820.2371\n",
      "Epoch [96/350], Loss: 16640.2969\n",
      "Epoch [97/350], Loss: 16634.7584\n",
      "Epoch [98/350], Loss: 16423.3775\n",
      "Epoch [99/350], Loss: 16456.0639\n",
      "Epoch [100/350], Loss: 16321.0449\n",
      "Epoch [101/350], Loss: 16202.3719\n",
      "Epoch [102/350], Loss: 16165.1698\n",
      "Epoch [103/350], Loss: 16000.6395\n",
      "Epoch [104/350], Loss: 15967.0136\n",
      "Epoch [105/350], Loss: 15839.0977\n",
      "Epoch [106/350], Loss: 15806.9730\n",
      "Epoch [107/350], Loss: 15685.5754\n",
      "Epoch [108/350], Loss: 15538.6973\n",
      "Epoch [109/350], Loss: 15457.0872\n",
      "Epoch [110/350], Loss: 15466.4407\n",
      "Epoch [111/350], Loss: 15382.1553\n",
      "Epoch [112/350], Loss: 15285.6686\n",
      "Epoch [113/350], Loss: 15206.7573\n",
      "Epoch [114/350], Loss: 15108.2389\n",
      "Epoch [115/350], Loss: 14996.7831\n",
      "Epoch [116/350], Loss: 14925.4055\n",
      "Epoch [117/350], Loss: 14842.8657\n",
      "Epoch [118/350], Loss: 14777.8859\n",
      "Epoch [119/350], Loss: 14711.9810\n",
      "Epoch [120/350], Loss: 14633.2687\n",
      "Epoch [121/350], Loss: 14505.9693\n",
      "Epoch [122/350], Loss: 14491.5007\n",
      "Epoch [123/350], Loss: 14368.5333\n",
      "Epoch [124/350], Loss: 14325.0042\n",
      "Epoch [125/350], Loss: 14224.2649\n",
      "Epoch [126/350], Loss: 14175.0876\n",
      "Epoch [127/350], Loss: 14150.5520\n",
      "Epoch [128/350], Loss: 14033.4245\n",
      "Epoch [129/350], Loss: 13943.6889\n",
      "Epoch [130/350], Loss: 13819.1801\n",
      "Epoch [131/350], Loss: 13853.3956\n",
      "Epoch [132/350], Loss: 13757.1763\n",
      "Epoch [133/350], Loss: 13640.3487\n",
      "Epoch [134/350], Loss: 13587.0176\n",
      "Epoch [135/350], Loss: 13487.5685\n",
      "Epoch [136/350], Loss: 13443.8328\n",
      "Epoch [137/350], Loss: 13425.5848\n",
      "Epoch [138/350], Loss: 13352.0421\n",
      "Epoch [139/350], Loss: 13246.2299\n",
      "Epoch [140/350], Loss: 13180.5922\n",
      "Epoch [141/350], Loss: 13092.2483\n",
      "Epoch [142/350], Loss: 13019.6368\n",
      "Epoch [143/350], Loss: 12944.7562\n",
      "Epoch [144/350], Loss: 12937.3840\n",
      "Epoch [145/350], Loss: 12792.6780\n",
      "Epoch [146/350], Loss: 12722.6057\n",
      "Epoch [147/350], Loss: 12730.3292\n",
      "Epoch [148/350], Loss: 12628.2482\n",
      "Epoch [149/350], Loss: 12537.2006\n",
      "Epoch [150/350], Loss: 12469.8663\n",
      "Epoch [151/350], Loss: 12491.3084\n",
      "Epoch [152/350], Loss: 12359.9074\n",
      "Epoch [153/350], Loss: 12400.3044\n",
      "Epoch [154/350], Loss: 12331.2295\n",
      "Epoch [155/350], Loss: 12193.6683\n",
      "Epoch [156/350], Loss: 12203.5092\n",
      "Epoch [157/350], Loss: 12141.5853\n",
      "Epoch [158/350], Loss: 12061.2419\n",
      "Epoch [159/350], Loss: 11986.5353\n",
      "Epoch [160/350], Loss: 11875.4192\n",
      "Epoch [161/350], Loss: 11864.1326\n",
      "Epoch [162/350], Loss: 11762.9614\n",
      "Epoch [163/350], Loss: 11805.7587\n",
      "Epoch [164/350], Loss: 11779.4647\n",
      "Epoch [165/350], Loss: 11608.7999\n",
      "Epoch [166/350], Loss: 11632.3505\n",
      "Epoch [167/350], Loss: 11549.4944\n",
      "Epoch [168/350], Loss: 11526.3955\n",
      "Epoch [169/350], Loss: 11477.2831\n",
      "Epoch [170/350], Loss: 11357.0149\n",
      "Epoch [171/350], Loss: 11321.3188\n",
      "Epoch [172/350], Loss: 11304.8713\n",
      "Epoch [173/350], Loss: 11184.3464\n",
      "Epoch [174/350], Loss: 11091.0538\n",
      "Epoch [175/350], Loss: 11099.3984\n",
      "Epoch [176/350], Loss: 11094.5498\n",
      "Epoch [177/350], Loss: 11024.2139\n",
      "Epoch [178/350], Loss: 11024.4688\n",
      "Epoch [179/350], Loss: 10942.2641\n",
      "Epoch [180/350], Loss: 10905.5532\n",
      "Epoch [181/350], Loss: 10871.9909\n",
      "Epoch [182/350], Loss: 10779.2322\n",
      "Epoch [183/350], Loss: 10638.5295\n",
      "Epoch [184/350], Loss: 10678.6058\n",
      "Epoch [185/350], Loss: 10630.7747\n",
      "Epoch [186/350], Loss: 10548.1480\n",
      "Epoch [187/350], Loss: 10532.7509\n",
      "Epoch [188/350], Loss: 10501.9955\n",
      "Epoch [189/350], Loss: 10471.9316\n",
      "Epoch [190/350], Loss: 10370.0951\n",
      "Epoch [191/350], Loss: 10373.9249\n",
      "Epoch [192/350], Loss: 10283.8344\n",
      "Epoch [193/350], Loss: 10273.5688\n",
      "Epoch [194/350], Loss: 10142.9828\n",
      "Epoch [195/350], Loss: 10212.7584\n",
      "Epoch [196/350], Loss: 10166.1926\n",
      "Epoch [197/350], Loss: 10068.5496\n",
      "Epoch [198/350], Loss: 10120.5156\n",
      "Epoch [199/350], Loss: 10067.3679\n",
      "Epoch [200/350], Loss: 10044.3832\n",
      "Epoch [201/350], Loss: 9941.5643\n",
      "Epoch [202/350], Loss: 9938.4807\n",
      "Epoch [203/350], Loss: 9842.6004\n",
      "Epoch [204/350], Loss: 9773.5292\n",
      "Epoch [205/350], Loss: 9726.2906\n",
      "Epoch [206/350], Loss: 9704.2719\n",
      "Epoch [207/350], Loss: 9667.6777\n",
      "Epoch [208/350], Loss: 9601.8887\n",
      "Epoch [209/350], Loss: 9673.0187\n",
      "Epoch [210/350], Loss: 9524.4200\n",
      "Epoch [211/350], Loss: 9567.7170\n",
      "Epoch [212/350], Loss: 9453.0186\n",
      "Epoch [213/350], Loss: 9452.6953\n",
      "Epoch [214/350], Loss: 9469.4552\n",
      "Epoch [215/350], Loss: 9334.8204\n",
      "Epoch [216/350], Loss: 9322.5453\n",
      "Epoch [217/350], Loss: 9275.2818\n",
      "Epoch [218/350], Loss: 9245.8704\n",
      "Epoch [219/350], Loss: 9210.0138\n",
      "Epoch [220/350], Loss: 9082.1277\n",
      "Epoch [221/350], Loss: 9128.4520\n",
      "Epoch [222/350], Loss: 9093.3738\n",
      "Epoch [223/350], Loss: 9017.9404\n",
      "Epoch [224/350], Loss: 8985.7297\n",
      "Epoch [225/350], Loss: 8920.9063\n",
      "Epoch [226/350], Loss: 8934.6816\n",
      "Epoch [227/350], Loss: 8954.0992\n",
      "Epoch [228/350], Loss: 8893.0345\n",
      "Epoch [229/350], Loss: 8763.5365\n",
      "Epoch [230/350], Loss: 8857.8150\n",
      "Epoch [231/350], Loss: 8797.5199\n",
      "Epoch [232/350], Loss: 8730.5937\n",
      "Epoch [233/350], Loss: 8638.1011\n",
      "Epoch [234/350], Loss: 8593.4042\n",
      "Epoch [235/350], Loss: 8699.6172\n",
      "Epoch [236/350], Loss: 8580.3834\n",
      "Epoch [237/350], Loss: 8520.3461\n",
      "Epoch [238/350], Loss: 8561.0164\n",
      "Epoch [239/350], Loss: 8467.8501\n",
      "Epoch [240/350], Loss: 8474.6626\n",
      "Epoch [241/350], Loss: 8454.3687\n",
      "Epoch [242/350], Loss: 8440.7285\n",
      "Epoch [243/350], Loss: 8435.3282\n",
      "Epoch [244/350], Loss: 8372.2965\n",
      "Epoch [245/350], Loss: 8253.9475\n",
      "Epoch [246/350], Loss: 8256.9838\n",
      "Epoch [247/350], Loss: 8300.5400\n",
      "Epoch [248/350], Loss: 8243.9052\n",
      "Epoch [249/350], Loss: 8143.7018\n",
      "Epoch [250/350], Loss: 8156.5632\n",
      "Epoch [251/350], Loss: 8135.8314\n",
      "Epoch [252/350], Loss: 8050.8169\n",
      "Epoch [253/350], Loss: 8063.6335\n",
      "Epoch [254/350], Loss: 8038.4399\n",
      "Epoch [255/350], Loss: 8017.7992\n",
      "Epoch [256/350], Loss: 7983.4947\n",
      "Epoch [257/350], Loss: 7945.6524\n",
      "Epoch [258/350], Loss: 7983.0493\n",
      "Epoch [259/350], Loss: 7813.3375\n",
      "Epoch [260/350], Loss: 7798.2129\n",
      "Epoch [261/350], Loss: 7817.2410\n",
      "Epoch [262/350], Loss: 7833.9077\n",
      "Epoch [263/350], Loss: 7670.5162\n",
      "Epoch [264/350], Loss: 7740.1497\n",
      "Epoch [265/350], Loss: 7742.6189\n",
      "Epoch [266/350], Loss: 7729.9595\n",
      "Epoch [267/350], Loss: 7602.2012\n",
      "Epoch [268/350], Loss: 7624.5807\n",
      "Epoch [269/350], Loss: 7634.9726\n",
      "Epoch [270/350], Loss: 7529.4099\n",
      "Epoch [271/350], Loss: 7526.1670\n",
      "Epoch [272/350], Loss: 7531.6219\n",
      "Epoch [273/350], Loss: 7479.0242\n",
      "Epoch [274/350], Loss: 7525.4164\n",
      "Epoch [275/350], Loss: 7517.2508\n",
      "Epoch [276/350], Loss: 7402.6577\n",
      "Epoch [277/350], Loss: 7418.4669\n",
      "Epoch [278/350], Loss: 7298.8278\n",
      "Epoch [279/350], Loss: 7337.7491\n",
      "Epoch [280/350], Loss: 7329.1723\n",
      "Epoch [281/350], Loss: 7324.3239\n",
      "Epoch [282/350], Loss: 7330.7394\n",
      "Epoch [283/350], Loss: 7233.7249\n",
      "Epoch [284/350], Loss: 7288.1427\n",
      "Epoch [285/350], Loss: 7197.1110\n",
      "Epoch [286/350], Loss: 7159.8683\n",
      "Epoch [287/350], Loss: 7128.8213\n",
      "Epoch [288/350], Loss: 7169.7123\n",
      "Epoch [289/350], Loss: 7063.1622\n",
      "Epoch [290/350], Loss: 6989.4929\n",
      "Epoch [291/350], Loss: 7015.9149\n",
      "Epoch [292/350], Loss: 7116.1978\n",
      "Epoch [293/350], Loss: 7032.7773\n",
      "Epoch [294/350], Loss: 6993.4041\n",
      "Epoch [295/350], Loss: 6920.3013\n",
      "Epoch [296/350], Loss: 6943.0318\n",
      "Epoch [297/350], Loss: 6870.6108\n",
      "Epoch [298/350], Loss: 6890.9586\n",
      "Epoch [299/350], Loss: 6859.6918\n",
      "Epoch [300/350], Loss: 6902.8862\n",
      "Epoch [301/350], Loss: 6777.0376\n",
      "Epoch [302/350], Loss: 6814.0755\n",
      "Epoch [303/350], Loss: 6808.8299\n",
      "Epoch [304/350], Loss: 6754.3635\n",
      "Epoch [305/350], Loss: 6646.8583\n",
      "Epoch [306/350], Loss: 6741.2797\n",
      "Epoch [307/350], Loss: 6680.7610\n",
      "Epoch [308/350], Loss: 6719.5855\n",
      "Epoch [309/350], Loss: 6649.2478\n",
      "Epoch [310/350], Loss: 6638.9071\n",
      "Epoch [311/350], Loss: 6644.4527\n",
      "Epoch [312/350], Loss: 6681.5750\n",
      "Epoch [313/350], Loss: 6563.1498\n",
      "Epoch [314/350], Loss: 6565.8334\n",
      "Epoch [315/350], Loss: 6462.1532\n",
      "Epoch [316/350], Loss: 6529.4805\n",
      "Epoch [317/350], Loss: 6526.4356\n",
      "Epoch [318/350], Loss: 6430.0521\n",
      "Epoch [319/350], Loss: 6484.8738\n",
      "Epoch [320/350], Loss: 6437.5452\n",
      "Epoch [321/350], Loss: 6394.1822\n",
      "Epoch [322/350], Loss: 6378.6592\n",
      "Epoch [323/350], Loss: 6401.4800\n",
      "Epoch [324/350], Loss: 6353.3575\n",
      "Epoch [325/350], Loss: 6315.3703\n",
      "Epoch [326/350], Loss: 6248.4290\n",
      "Epoch [327/350], Loss: 6319.8771\n",
      "Epoch [328/350], Loss: 6241.6631\n",
      "Epoch [329/350], Loss: 6228.8871\n",
      "Epoch [330/350], Loss: 6248.0655\n",
      "Epoch [331/350], Loss: 6278.3465\n",
      "Epoch [332/350], Loss: 6185.6917\n",
      "Epoch [333/350], Loss: 6137.3687\n",
      "Epoch [334/350], Loss: 6185.1609\n",
      "Epoch [335/350], Loss: 6202.8668\n",
      "Epoch [336/350], Loss: 6114.7572\n",
      "Epoch [337/350], Loss: 6102.0858\n",
      "Epoch [338/350], Loss: 6102.9469\n",
      "Epoch [339/350], Loss: 6100.1856\n",
      "Epoch [340/350], Loss: 5958.8852\n",
      "Epoch [341/350], Loss: 6024.2765\n",
      "Epoch [342/350], Loss: 6001.3817\n",
      "Epoch [343/350], Loss: 5959.3708\n",
      "Epoch [344/350], Loss: 5991.4091\n",
      "Epoch [345/350], Loss: 5966.9774\n",
      "Epoch [346/350], Loss: 5922.9606\n",
      "Epoch [347/350], Loss: 5963.4205\n",
      "Epoch [348/350], Loss: 5877.7967\n",
      "Epoch [349/350], Loss: 5872.1436\n",
      "Epoch [350/350], Loss: 5871.0361\n",
      "Epoch [1/350], Loss: 55159.3188\n",
      "Epoch [2/350], Loss: 51682.2759\n",
      "Epoch [3/350], Loss: 49367.9678\n",
      "Epoch [4/350], Loss: 47808.6406\n",
      "Epoch [5/350], Loss: 46753.8188\n",
      "Epoch [6/350], Loss: 46050.9819\n",
      "Epoch [7/350], Loss: 45375.1882\n",
      "Epoch [8/350], Loss: 44841.3455\n",
      "Epoch [9/350], Loss: 44324.1355\n",
      "Epoch [10/350], Loss: 43864.1785\n",
      "Epoch [11/350], Loss: 43336.2463\n",
      "Epoch [12/350], Loss: 42895.5002\n",
      "Epoch [13/350], Loss: 42539.6450\n",
      "Epoch [14/350], Loss: 42026.3098\n",
      "Epoch [15/350], Loss: 41656.3108\n",
      "Epoch [16/350], Loss: 41327.6633\n",
      "Epoch [17/350], Loss: 40899.8757\n",
      "Epoch [18/350], Loss: 40455.5500\n",
      "Epoch [19/350], Loss: 40128.4404\n",
      "Epoch [20/350], Loss: 39773.1248\n",
      "Epoch [21/350], Loss: 39393.8540\n",
      "Epoch [22/350], Loss: 39013.8374\n",
      "Epoch [23/350], Loss: 38619.7251\n",
      "Epoch [24/350], Loss: 38311.5718\n",
      "Epoch [25/350], Loss: 38008.3408\n",
      "Epoch [26/350], Loss: 37676.5356\n",
      "Epoch [27/350], Loss: 37332.8604\n",
      "Epoch [28/350], Loss: 37015.8677\n",
      "Epoch [29/350], Loss: 36663.6917\n",
      "Epoch [30/350], Loss: 36410.9116\n",
      "Epoch [31/350], Loss: 36075.8020\n",
      "Epoch [32/350], Loss: 35727.3318\n",
      "Epoch [33/350], Loss: 35404.8464\n",
      "Epoch [34/350], Loss: 35138.6411\n",
      "Epoch [35/350], Loss: 34910.7769\n",
      "Epoch [36/350], Loss: 34649.4438\n",
      "Epoch [37/350], Loss: 34345.7319\n",
      "Epoch [38/350], Loss: 34038.2502\n",
      "Epoch [39/350], Loss: 33885.6145\n",
      "Epoch [40/350], Loss: 33491.8712\n",
      "Epoch [41/350], Loss: 33240.6664\n",
      "Epoch [42/350], Loss: 33095.2623\n",
      "Epoch [43/350], Loss: 32761.2334\n",
      "Epoch [44/350], Loss: 32466.3322\n",
      "Epoch [45/350], Loss: 32164.6486\n",
      "Epoch [46/350], Loss: 32002.6787\n",
      "Epoch [47/350], Loss: 31825.5197\n",
      "Epoch [48/350], Loss: 31496.4518\n",
      "Epoch [49/350], Loss: 31310.7028\n",
      "Epoch [50/350], Loss: 31017.6500\n",
      "Epoch [51/350], Loss: 30822.7289\n",
      "Epoch [52/350], Loss: 30556.4514\n",
      "Epoch [53/350], Loss: 30301.1835\n",
      "Epoch [54/350], Loss: 30136.8013\n",
      "Epoch [55/350], Loss: 29942.4266\n",
      "Epoch [56/350], Loss: 29698.9369\n",
      "Epoch [57/350], Loss: 29443.6934\n",
      "Epoch [58/350], Loss: 29280.9369\n",
      "Epoch [59/350], Loss: 29081.7155\n",
      "Epoch [60/350], Loss: 28862.7214\n",
      "Epoch [61/350], Loss: 28703.8445\n",
      "Epoch [62/350], Loss: 28473.3069\n",
      "Epoch [63/350], Loss: 28240.5627\n",
      "Epoch [64/350], Loss: 28060.3763\n",
      "Epoch [65/350], Loss: 27721.9487\n",
      "Epoch [66/350], Loss: 27747.0128\n",
      "Epoch [67/350], Loss: 27493.8767\n",
      "Epoch [68/350], Loss: 27249.0785\n",
      "Epoch [69/350], Loss: 27108.2078\n",
      "Epoch [70/350], Loss: 26907.5696\n",
      "Epoch [71/350], Loss: 26783.0980\n",
      "Epoch [72/350], Loss: 26513.7555\n",
      "Epoch [73/350], Loss: 26445.7881\n",
      "Epoch [74/350], Loss: 26204.1628\n",
      "Epoch [75/350], Loss: 26061.9965\n",
      "Epoch [76/350], Loss: 25825.8087\n",
      "Epoch [77/350], Loss: 25635.3844\n",
      "Epoch [78/350], Loss: 25523.7316\n",
      "Epoch [79/350], Loss: 25435.4204\n",
      "Epoch [80/350], Loss: 25128.3413\n",
      "Epoch [81/350], Loss: 24989.4230\n",
      "Epoch [82/350], Loss: 24836.5483\n",
      "Epoch [83/350], Loss: 24692.2546\n",
      "Epoch [84/350], Loss: 24567.5334\n",
      "Epoch [85/350], Loss: 24344.8923\n",
      "Epoch [86/350], Loss: 24158.3792\n",
      "Epoch [87/350], Loss: 24032.4366\n",
      "Epoch [88/350], Loss: 23933.6351\n",
      "Epoch [89/350], Loss: 23650.7847\n",
      "Epoch [90/350], Loss: 23606.9327\n",
      "Epoch [91/350], Loss: 23460.0071\n",
      "Epoch [92/350], Loss: 23197.1195\n",
      "Epoch [93/350], Loss: 23078.6042\n",
      "Epoch [94/350], Loss: 22963.5747\n",
      "Epoch [95/350], Loss: 22809.5574\n",
      "Epoch [96/350], Loss: 22647.0074\n",
      "Epoch [97/350], Loss: 22547.7582\n",
      "Epoch [98/350], Loss: 22419.3146\n",
      "Epoch [99/350], Loss: 22292.8303\n",
      "Epoch [100/350], Loss: 22147.5645\n",
      "Epoch [101/350], Loss: 22010.9219\n",
      "Epoch [102/350], Loss: 21843.4517\n",
      "Epoch [103/350], Loss: 21663.3958\n",
      "Epoch [104/350], Loss: 21582.3370\n",
      "Epoch [105/350], Loss: 21399.0834\n",
      "Epoch [106/350], Loss: 21378.9592\n",
      "Epoch [107/350], Loss: 21202.2743\n",
      "Epoch [108/350], Loss: 21079.8104\n",
      "Epoch [109/350], Loss: 20906.2410\n",
      "Epoch [110/350], Loss: 20806.6661\n",
      "Epoch [111/350], Loss: 20674.0520\n",
      "Epoch [112/350], Loss: 20634.0571\n",
      "Epoch [113/350], Loss: 20409.6396\n",
      "Epoch [114/350], Loss: 20357.0521\n",
      "Epoch [115/350], Loss: 20276.5436\n",
      "Epoch [116/350], Loss: 20114.2849\n",
      "Epoch [117/350], Loss: 19928.9181\n",
      "Epoch [118/350], Loss: 19855.3745\n",
      "Epoch [119/350], Loss: 19713.7333\n",
      "Epoch [120/350], Loss: 19632.2548\n",
      "Epoch [121/350], Loss: 19509.5308\n",
      "Epoch [122/350], Loss: 19434.7092\n",
      "Epoch [123/350], Loss: 19335.0437\n",
      "Epoch [124/350], Loss: 19159.8075\n",
      "Epoch [125/350], Loss: 19040.8158\n",
      "Epoch [126/350], Loss: 18950.2407\n",
      "Epoch [127/350], Loss: 18824.2170\n",
      "Epoch [128/350], Loss: 18680.0020\n",
      "Epoch [129/350], Loss: 18630.0916\n",
      "Epoch [130/350], Loss: 18649.6469\n",
      "Epoch [131/350], Loss: 18375.4100\n",
      "Epoch [132/350], Loss: 18263.4254\n",
      "Epoch [133/350], Loss: 18193.1429\n",
      "Epoch [134/350], Loss: 18043.6510\n",
      "Epoch [135/350], Loss: 17938.8452\n",
      "Epoch [136/350], Loss: 17871.0339\n",
      "Epoch [137/350], Loss: 17735.7216\n",
      "Epoch [138/350], Loss: 17662.8074\n",
      "Epoch [139/350], Loss: 17536.5984\n",
      "Epoch [140/350], Loss: 17407.7047\n",
      "Epoch [141/350], Loss: 17389.4492\n",
      "Epoch [142/350], Loss: 17301.5499\n",
      "Epoch [143/350], Loss: 17250.4659\n",
      "Epoch [144/350], Loss: 17076.3259\n",
      "Epoch [145/350], Loss: 16981.4410\n",
      "Epoch [146/350], Loss: 16860.3077\n",
      "Epoch [147/350], Loss: 16755.9655\n",
      "Epoch [148/350], Loss: 16753.4800\n",
      "Epoch [149/350], Loss: 16694.7089\n",
      "Epoch [150/350], Loss: 16468.6501\n",
      "Epoch [151/350], Loss: 16400.4462\n",
      "Epoch [152/350], Loss: 16297.2756\n",
      "Epoch [153/350], Loss: 16271.2391\n",
      "Epoch [154/350], Loss: 16277.1644\n",
      "Epoch [155/350], Loss: 16123.3901\n",
      "Epoch [156/350], Loss: 16053.4984\n",
      "Epoch [157/350], Loss: 15893.7915\n",
      "Epoch [158/350], Loss: 15878.7606\n",
      "Epoch [159/350], Loss: 15789.2615\n",
      "Epoch [160/350], Loss: 15628.5693\n",
      "Epoch [161/350], Loss: 15643.6336\n",
      "Epoch [162/350], Loss: 15411.1315\n",
      "Epoch [163/350], Loss: 15444.6868\n",
      "Epoch [164/350], Loss: 15429.3997\n",
      "Epoch [165/350], Loss: 15331.2384\n",
      "Epoch [166/350], Loss: 15221.2021\n",
      "Epoch [167/350], Loss: 15190.2620\n",
      "Epoch [168/350], Loss: 15100.5696\n",
      "Epoch [169/350], Loss: 14985.7587\n",
      "Epoch [170/350], Loss: 14841.3737\n",
      "Epoch [171/350], Loss: 14799.9945\n",
      "Epoch [172/350], Loss: 14691.4471\n",
      "Epoch [173/350], Loss: 14707.3286\n",
      "Epoch [174/350], Loss: 14619.9011\n",
      "Epoch [175/350], Loss: 14495.8453\n",
      "Epoch [176/350], Loss: 14481.7698\n",
      "Epoch [177/350], Loss: 14347.5165\n",
      "Epoch [178/350], Loss: 14270.7998\n",
      "Epoch [179/350], Loss: 14305.9562\n",
      "Epoch [180/350], Loss: 14241.3358\n",
      "Epoch [181/350], Loss: 14068.3108\n",
      "Epoch [182/350], Loss: 13959.5212\n",
      "Epoch [183/350], Loss: 13895.9733\n",
      "Epoch [184/350], Loss: 13841.0916\n",
      "Epoch [185/350], Loss: 13854.8767\n",
      "Epoch [186/350], Loss: 13670.8526\n",
      "Epoch [187/350], Loss: 13663.1032\n",
      "Epoch [188/350], Loss: 13656.8687\n",
      "Epoch [189/350], Loss: 13537.6608\n",
      "Epoch [190/350], Loss: 13527.7789\n",
      "Epoch [191/350], Loss: 13345.7670\n",
      "Epoch [192/350], Loss: 13432.6779\n",
      "Epoch [193/350], Loss: 13241.0912\n",
      "Epoch [194/350], Loss: 13138.9904\n",
      "Epoch [195/350], Loss: 13141.3765\n",
      "Epoch [196/350], Loss: 13121.0225\n",
      "Epoch [197/350], Loss: 13005.0731\n",
      "Epoch [198/350], Loss: 12898.4122\n",
      "Epoch [199/350], Loss: 12869.0368\n",
      "Epoch [200/350], Loss: 12742.8641\n",
      "Epoch [201/350], Loss: 12796.9407\n",
      "Epoch [202/350], Loss: 12701.8795\n",
      "Epoch [203/350], Loss: 12633.8465\n",
      "Epoch [204/350], Loss: 12568.6790\n",
      "Epoch [205/350], Loss: 12466.6245\n",
      "Epoch [206/350], Loss: 12431.9102\n",
      "Epoch [207/350], Loss: 12362.2257\n",
      "Epoch [208/350], Loss: 12355.8813\n",
      "Epoch [209/350], Loss: 12253.2454\n",
      "Epoch [210/350], Loss: 12182.7017\n",
      "Epoch [211/350], Loss: 12158.1996\n",
      "Epoch [212/350], Loss: 12120.4860\n",
      "Epoch [213/350], Loss: 12105.9609\n",
      "Epoch [214/350], Loss: 11995.3474\n",
      "Epoch [215/350], Loss: 11871.8884\n",
      "Epoch [216/350], Loss: 11850.0661\n",
      "Epoch [217/350], Loss: 11821.6284\n",
      "Epoch [218/350], Loss: 11767.5684\n",
      "Epoch [219/350], Loss: 11772.2678\n",
      "Epoch [220/350], Loss: 11698.9585\n",
      "Epoch [221/350], Loss: 11577.2186\n",
      "Epoch [222/350], Loss: 11552.9459\n",
      "Epoch [223/350], Loss: 11583.9722\n",
      "Epoch [224/350], Loss: 11437.4109\n",
      "Epoch [225/350], Loss: 11341.3913\n",
      "Epoch [226/350], Loss: 11260.5356\n",
      "Epoch [227/350], Loss: 11259.6125\n",
      "Epoch [228/350], Loss: 11220.6703\n",
      "Epoch [229/350], Loss: 11205.0158\n",
      "Epoch [230/350], Loss: 11156.6074\n",
      "Epoch [231/350], Loss: 11036.1023\n",
      "Epoch [232/350], Loss: 10958.9670\n",
      "Epoch [233/350], Loss: 10958.6831\n",
      "Epoch [234/350], Loss: 10946.7435\n",
      "Epoch [235/350], Loss: 10888.7234\n",
      "Epoch [236/350], Loss: 10835.9335\n",
      "Epoch [237/350], Loss: 10798.4372\n",
      "Epoch [238/350], Loss: 10772.3185\n",
      "Epoch [239/350], Loss: 10721.9022\n",
      "Epoch [240/350], Loss: 10768.9589\n",
      "Epoch [241/350], Loss: 10623.5782\n",
      "Epoch [242/350], Loss: 10585.6161\n",
      "Epoch [243/350], Loss: 10508.5806\n",
      "Epoch [244/350], Loss: 10432.7316\n",
      "Epoch [245/350], Loss: 10388.7377\n",
      "Epoch [246/350], Loss: 10453.1995\n",
      "Epoch [247/350], Loss: 10379.1573\n",
      "Epoch [248/350], Loss: 10286.0183\n",
      "Epoch [249/350], Loss: 10312.3765\n",
      "Epoch [250/350], Loss: 10200.0104\n",
      "Epoch [251/350], Loss: 10207.3762\n",
      "Epoch [252/350], Loss: 10106.3598\n",
      "Epoch [253/350], Loss: 10030.4650\n",
      "Epoch [254/350], Loss: 10029.1516\n",
      "Epoch [255/350], Loss: 9995.5338\n",
      "Epoch [256/350], Loss: 9909.0070\n",
      "Epoch [257/350], Loss: 9984.5009\n",
      "Epoch [258/350], Loss: 9835.9243\n",
      "Epoch [259/350], Loss: 9867.5057\n",
      "Epoch [260/350], Loss: 9802.6403\n",
      "Epoch [261/350], Loss: 9771.7131\n",
      "Epoch [262/350], Loss: 9678.0481\n",
      "Epoch [263/350], Loss: 9689.3480\n",
      "Epoch [264/350], Loss: 9687.3319\n",
      "Epoch [265/350], Loss: 9547.6868\n",
      "Epoch [266/350], Loss: 9540.8302\n",
      "Epoch [267/350], Loss: 9484.9844\n",
      "Epoch [268/350], Loss: 9516.0526\n",
      "Epoch [269/350], Loss: 9445.5776\n",
      "Epoch [270/350], Loss: 9340.7712\n",
      "Epoch [271/350], Loss: 9259.0500\n",
      "Epoch [272/350], Loss: 9333.7054\n",
      "Epoch [273/350], Loss: 9222.8074\n",
      "Epoch [274/350], Loss: 9192.9739\n",
      "Epoch [275/350], Loss: 9296.5684\n",
      "Epoch [276/350], Loss: 9174.3474\n",
      "Epoch [277/350], Loss: 9194.7947\n",
      "Epoch [278/350], Loss: 9084.4147\n",
      "Epoch [279/350], Loss: 9075.5121\n",
      "Epoch [280/350], Loss: 9003.2647\n",
      "Epoch [281/350], Loss: 9027.2382\n",
      "Epoch [282/350], Loss: 8963.7853\n",
      "Epoch [283/350], Loss: 8951.1896\n",
      "Epoch [284/350], Loss: 8880.5426\n",
      "Epoch [285/350], Loss: 8873.3176\n",
      "Epoch [286/350], Loss: 8829.9913\n",
      "Epoch [287/350], Loss: 8765.5978\n",
      "Epoch [288/350], Loss: 8668.2318\n",
      "Epoch [289/350], Loss: 8621.4081\n",
      "Epoch [290/350], Loss: 8680.2397\n",
      "Epoch [291/350], Loss: 8522.2627\n",
      "Epoch [292/350], Loss: 8647.6833\n",
      "Epoch [293/350], Loss: 8539.4706\n",
      "Epoch [294/350], Loss: 8469.5167\n",
      "Epoch [295/350], Loss: 8452.6180\n",
      "Epoch [296/350], Loss: 8432.5545\n",
      "Epoch [297/350], Loss: 8418.6395\n",
      "Epoch [298/350], Loss: 8353.1777\n",
      "Epoch [299/350], Loss: 8432.0138\n",
      "Epoch [300/350], Loss: 8354.7810\n",
      "Epoch [301/350], Loss: 8303.6327\n",
      "Epoch [302/350], Loss: 8218.5689\n",
      "Epoch [303/350], Loss: 8214.9415\n",
      "Epoch [304/350], Loss: 8244.8534\n",
      "Epoch [305/350], Loss: 8188.3251\n",
      "Epoch [306/350], Loss: 8212.3178\n",
      "Epoch [307/350], Loss: 8077.5575\n",
      "Epoch [308/350], Loss: 8148.3875\n",
      "Epoch [309/350], Loss: 8039.2628\n",
      "Epoch [310/350], Loss: 8001.2821\n",
      "Epoch [311/350], Loss: 8056.1010\n",
      "Epoch [312/350], Loss: 7928.5186\n",
      "Epoch [313/350], Loss: 7997.7549\n",
      "Epoch [314/350], Loss: 7926.9183\n",
      "Epoch [315/350], Loss: 7875.6003\n",
      "Epoch [316/350], Loss: 7827.3130\n",
      "Epoch [317/350], Loss: 7848.9090\n",
      "Epoch [318/350], Loss: 7784.2896\n",
      "Epoch [319/350], Loss: 7854.9002\n",
      "Epoch [320/350], Loss: 7712.9167\n",
      "Epoch [321/350], Loss: 7766.8539\n",
      "Epoch [322/350], Loss: 7726.9248\n",
      "Epoch [323/350], Loss: 7714.2587\n",
      "Epoch [324/350], Loss: 7739.7701\n",
      "Epoch [325/350], Loss: 7559.2919\n",
      "Epoch [326/350], Loss: 7645.0492\n",
      "Epoch [327/350], Loss: 7572.7426\n",
      "Epoch [328/350], Loss: 7580.1205\n",
      "Epoch [329/350], Loss: 7560.1942\n",
      "Epoch [330/350], Loss: 7505.6212\n",
      "Epoch [331/350], Loss: 7439.3619\n",
      "Epoch [332/350], Loss: 7424.0066\n",
      "Epoch [333/350], Loss: 7388.5549\n",
      "Epoch [334/350], Loss: 7417.9960\n",
      "Epoch [335/350], Loss: 7362.6269\n",
      "Epoch [336/350], Loss: 7260.9519\n",
      "Epoch [337/350], Loss: 7340.0377\n",
      "Epoch [338/350], Loss: 7278.1568\n",
      "Epoch [339/350], Loss: 7304.3712\n",
      "Epoch [340/350], Loss: 7313.5675\n",
      "Epoch [341/350], Loss: 7269.9382\n",
      "Epoch [342/350], Loss: 7259.5145\n",
      "Epoch [343/350], Loss: 7245.7250\n",
      "Epoch [344/350], Loss: 7108.2874\n",
      "Epoch [345/350], Loss: 7176.7562\n",
      "Epoch [346/350], Loss: 7155.4289\n",
      "Epoch [347/350], Loss: 7087.6911\n",
      "Epoch [348/350], Loss: 7069.6913\n",
      "Epoch [349/350], Loss: 7048.3562\n",
      "Epoch [350/350], Loss: 7067.1384\n",
      "Epoch [1/350], Loss: 84078.9756\n",
      "Epoch [2/350], Loss: 80061.4463\n",
      "Epoch [3/350], Loss: 77217.4951\n",
      "Epoch [4/350], Loss: 75183.2661\n",
      "Epoch [5/350], Loss: 73874.7607\n",
      "Epoch [6/350], Loss: 72769.4800\n",
      "Epoch [7/350], Loss: 71841.8247\n",
      "Epoch [8/350], Loss: 70928.6665\n",
      "Epoch [9/350], Loss: 70116.2549\n",
      "Epoch [10/350], Loss: 69352.6372\n",
      "Epoch [11/350], Loss: 68571.1201\n",
      "Epoch [12/350], Loss: 67869.7998\n",
      "Epoch [13/350], Loss: 67135.2812\n",
      "Epoch [14/350], Loss: 66561.5063\n",
      "Epoch [15/350], Loss: 65772.2021\n",
      "Epoch [16/350], Loss: 65148.8098\n",
      "Epoch [17/350], Loss: 64543.6279\n",
      "Epoch [18/350], Loss: 63949.5784\n",
      "Epoch [19/350], Loss: 63254.9731\n",
      "Epoch [20/350], Loss: 62652.8325\n",
      "Epoch [21/350], Loss: 62043.0557\n",
      "Epoch [22/350], Loss: 61522.4285\n",
      "Epoch [23/350], Loss: 60991.3279\n",
      "Epoch [24/350], Loss: 60359.5098\n",
      "Epoch [25/350], Loss: 59859.3987\n",
      "Epoch [26/350], Loss: 59188.8000\n",
      "Epoch [27/350], Loss: 58773.0156\n",
      "Epoch [28/350], Loss: 58234.4775\n",
      "Epoch [29/350], Loss: 57814.8303\n",
      "Epoch [30/350], Loss: 57162.9160\n",
      "Epoch [31/350], Loss: 56727.6174\n",
      "Epoch [32/350], Loss: 56240.4890\n",
      "Epoch [33/350], Loss: 55754.0479\n",
      "Epoch [34/350], Loss: 55280.0996\n",
      "Epoch [35/350], Loss: 54771.1165\n",
      "Epoch [36/350], Loss: 54365.6113\n",
      "Epoch [37/350], Loss: 53892.2947\n",
      "Epoch [38/350], Loss: 53467.5417\n",
      "Epoch [39/350], Loss: 52922.1228\n",
      "Epoch [40/350], Loss: 52514.8738\n",
      "Epoch [41/350], Loss: 52151.7839\n",
      "Epoch [42/350], Loss: 51603.3657\n",
      "Epoch [43/350], Loss: 51306.9441\n",
      "Epoch [44/350], Loss: 50830.4792\n",
      "Epoch [45/350], Loss: 50438.0625\n",
      "Epoch [46/350], Loss: 50013.7202\n",
      "Epoch [47/350], Loss: 49688.9412\n",
      "Epoch [48/350], Loss: 49285.0698\n",
      "Epoch [49/350], Loss: 48844.5476\n",
      "Epoch [50/350], Loss: 48563.7224\n",
      "Epoch [51/350], Loss: 48039.7881\n",
      "Epoch [52/350], Loss: 47693.5742\n",
      "Epoch [53/350], Loss: 47248.2378\n",
      "Epoch [54/350], Loss: 46941.0320\n",
      "Epoch [55/350], Loss: 46619.9597\n",
      "Epoch [56/350], Loss: 46228.5669\n",
      "Epoch [57/350], Loss: 45926.0129\n",
      "Epoch [58/350], Loss: 45690.1958\n",
      "Epoch [59/350], Loss: 45236.9409\n",
      "Epoch [60/350], Loss: 44883.5164\n",
      "Epoch [61/350], Loss: 44518.7329\n",
      "Epoch [62/350], Loss: 44211.0750\n",
      "Epoch [63/350], Loss: 43984.3953\n",
      "Epoch [64/350], Loss: 43500.1125\n",
      "Epoch [65/350], Loss: 43277.0737\n",
      "Epoch [66/350], Loss: 42865.6448\n",
      "Epoch [67/350], Loss: 42588.0303\n",
      "Epoch [68/350], Loss: 42271.2312\n",
      "Epoch [69/350], Loss: 41940.9419\n",
      "Epoch [70/350], Loss: 41717.3037\n",
      "Epoch [71/350], Loss: 41394.6611\n",
      "Epoch [72/350], Loss: 41072.4961\n",
      "Epoch [73/350], Loss: 40797.9238\n",
      "Epoch [74/350], Loss: 40473.3772\n",
      "Epoch [75/350], Loss: 40137.3733\n",
      "Epoch [76/350], Loss: 39816.7693\n",
      "Epoch [77/350], Loss: 39589.4824\n",
      "Epoch [78/350], Loss: 39306.1853\n",
      "Epoch [79/350], Loss: 39040.3291\n",
      "Epoch [80/350], Loss: 38741.2825\n",
      "Epoch [81/350], Loss: 38573.8533\n",
      "Epoch [82/350], Loss: 38280.5745\n",
      "Epoch [83/350], Loss: 37958.4260\n",
      "Epoch [84/350], Loss: 37758.1782\n",
      "Epoch [85/350], Loss: 37380.4143\n",
      "Epoch [86/350], Loss: 37144.5361\n",
      "Epoch [87/350], Loss: 36927.8047\n",
      "Epoch [88/350], Loss: 36623.3503\n",
      "Epoch [89/350], Loss: 36431.2810\n",
      "Epoch [90/350], Loss: 36164.0598\n",
      "Epoch [91/350], Loss: 35934.0127\n",
      "Epoch [92/350], Loss: 35722.4170\n",
      "Epoch [93/350], Loss: 35418.0488\n",
      "Epoch [94/350], Loss: 35042.2114\n",
      "Epoch [95/350], Loss: 34969.5571\n",
      "Epoch [96/350], Loss: 34630.2090\n",
      "Epoch [97/350], Loss: 34346.9302\n",
      "Epoch [98/350], Loss: 34234.3508\n",
      "Epoch [99/350], Loss: 33946.1812\n",
      "Epoch [100/350], Loss: 33690.9463\n",
      "Epoch [101/350], Loss: 33478.1426\n",
      "Epoch [102/350], Loss: 33277.1920\n",
      "Epoch [103/350], Loss: 33068.4307\n",
      "Epoch [104/350], Loss: 32839.9296\n",
      "Epoch [105/350], Loss: 32665.9146\n",
      "Epoch [106/350], Loss: 32426.0214\n",
      "Epoch [107/350], Loss: 32181.4412\n",
      "Epoch [108/350], Loss: 31954.6814\n",
      "Epoch [109/350], Loss: 31750.5619\n",
      "Epoch [110/350], Loss: 31562.8516\n",
      "Epoch [111/350], Loss: 31450.1339\n",
      "Epoch [112/350], Loss: 31106.4027\n",
      "Epoch [113/350], Loss: 30996.3037\n",
      "Epoch [114/350], Loss: 30679.2163\n",
      "Epoch [115/350], Loss: 30547.3759\n",
      "Epoch [116/350], Loss: 30354.6571\n",
      "Epoch [117/350], Loss: 30189.7852\n",
      "Epoch [118/350], Loss: 29959.0048\n",
      "Epoch [119/350], Loss: 29763.9634\n",
      "Epoch [120/350], Loss: 29560.2627\n",
      "Epoch [121/350], Loss: 29341.8420\n",
      "Epoch [122/350], Loss: 29202.7389\n",
      "Epoch [123/350], Loss: 28978.2440\n",
      "Epoch [124/350], Loss: 28793.9551\n",
      "Epoch [125/350], Loss: 28659.3021\n",
      "Epoch [126/350], Loss: 28476.0475\n",
      "Epoch [127/350], Loss: 28293.6741\n",
      "Epoch [128/350], Loss: 28061.9263\n",
      "Epoch [129/350], Loss: 27922.0094\n",
      "Epoch [130/350], Loss: 27738.4586\n",
      "Epoch [131/350], Loss: 27522.5746\n",
      "Epoch [132/350], Loss: 27450.2756\n",
      "Epoch [133/350], Loss: 27198.7941\n",
      "Epoch [134/350], Loss: 26993.7411\n",
      "Epoch [135/350], Loss: 26767.2361\n",
      "Epoch [136/350], Loss: 26768.9694\n",
      "Epoch [137/350], Loss: 26532.2241\n",
      "Epoch [138/350], Loss: 26393.7516\n",
      "Epoch [139/350], Loss: 26241.3219\n",
      "Epoch [140/350], Loss: 25997.3992\n",
      "Epoch [141/350], Loss: 25924.0476\n",
      "Epoch [142/350], Loss: 25608.4730\n",
      "Epoch [143/350], Loss: 25584.3761\n",
      "Epoch [144/350], Loss: 25393.7223\n",
      "Epoch [145/350], Loss: 25214.0576\n",
      "Epoch [146/350], Loss: 25121.6510\n",
      "Epoch [147/350], Loss: 24930.5431\n",
      "Epoch [148/350], Loss: 24764.4724\n",
      "Epoch [149/350], Loss: 24717.8982\n",
      "Epoch [150/350], Loss: 24464.1091\n",
      "Epoch [151/350], Loss: 24256.6263\n",
      "Epoch [152/350], Loss: 24075.7220\n",
      "Epoch [153/350], Loss: 24080.9434\n",
      "Epoch [154/350], Loss: 23876.7804\n",
      "Epoch [155/350], Loss: 23756.4500\n",
      "Epoch [156/350], Loss: 23642.3434\n",
      "Epoch [157/350], Loss: 23373.3218\n",
      "Epoch [158/350], Loss: 23311.4519\n",
      "Epoch [159/350], Loss: 23215.9634\n",
      "Epoch [160/350], Loss: 22948.0745\n",
      "Epoch [161/350], Loss: 22801.5081\n",
      "Epoch [162/350], Loss: 22771.2139\n",
      "Epoch [163/350], Loss: 22630.3931\n",
      "Epoch [164/350], Loss: 22457.3514\n",
      "Epoch [165/350], Loss: 22388.8049\n",
      "Epoch [166/350], Loss: 22218.1482\n",
      "Epoch [167/350], Loss: 22119.5361\n",
      "Epoch [168/350], Loss: 21916.6858\n",
      "Epoch [169/350], Loss: 21843.4132\n",
      "Epoch [170/350], Loss: 21664.4127\n",
      "Epoch [171/350], Loss: 21529.0116\n",
      "Epoch [172/350], Loss: 21557.9751\n",
      "Epoch [173/350], Loss: 21306.9215\n",
      "Epoch [174/350], Loss: 21190.9381\n",
      "Epoch [175/350], Loss: 21139.1975\n",
      "Epoch [176/350], Loss: 21006.6205\n",
      "Epoch [177/350], Loss: 20768.7404\n",
      "Epoch [178/350], Loss: 20661.2177\n",
      "Epoch [179/350], Loss: 20641.9801\n",
      "Epoch [180/350], Loss: 20551.0765\n",
      "Epoch [181/350], Loss: 20355.9482\n",
      "Epoch [182/350], Loss: 20268.7404\n",
      "Epoch [183/350], Loss: 20096.1239\n",
      "Epoch [184/350], Loss: 20079.1896\n",
      "Epoch [185/350], Loss: 19990.3500\n",
      "Epoch [186/350], Loss: 19806.3948\n",
      "Epoch [187/350], Loss: 19664.0170\n",
      "Epoch [188/350], Loss: 19565.9325\n",
      "Epoch [189/350], Loss: 19368.3146\n",
      "Epoch [190/350], Loss: 19306.4818\n",
      "Epoch [191/350], Loss: 19196.9789\n",
      "Epoch [192/350], Loss: 19107.2639\n",
      "Epoch [193/350], Loss: 19031.2158\n",
      "Epoch [194/350], Loss: 18909.8353\n",
      "Epoch [195/350], Loss: 18789.0858\n",
      "Epoch [196/350], Loss: 18779.6299\n",
      "Epoch [197/350], Loss: 18690.9783\n",
      "Epoch [198/350], Loss: 18467.1826\n",
      "Epoch [199/350], Loss: 18338.9167\n",
      "Epoch [200/350], Loss: 18362.6573\n",
      "Epoch [201/350], Loss: 18270.4694\n",
      "Epoch [202/350], Loss: 18150.3989\n",
      "Epoch [203/350], Loss: 17977.3485\n",
      "Epoch [204/350], Loss: 17876.9941\n",
      "Epoch [205/350], Loss: 17829.8281\n",
      "Epoch [206/350], Loss: 17729.2202\n",
      "Epoch [207/350], Loss: 17614.3616\n",
      "Epoch [208/350], Loss: 17455.6030\n",
      "Epoch [209/350], Loss: 17323.4240\n",
      "Epoch [210/350], Loss: 17368.6162\n",
      "Epoch [211/350], Loss: 17204.2145\n",
      "Epoch [212/350], Loss: 17063.8668\n",
      "Epoch [213/350], Loss: 17054.7523\n",
      "Epoch [214/350], Loss: 17024.8616\n",
      "Epoch [215/350], Loss: 16849.8614\n",
      "Epoch [216/350], Loss: 16638.7667\n",
      "Epoch [217/350], Loss: 16694.4700\n",
      "Epoch [218/350], Loss: 16548.6708\n",
      "Epoch [219/350], Loss: 16461.5883\n",
      "Epoch [220/350], Loss: 16382.3505\n",
      "Epoch [221/350], Loss: 16298.1603\n",
      "Epoch [222/350], Loss: 16176.8323\n",
      "Epoch [223/350], Loss: 16132.6070\n",
      "Epoch [224/350], Loss: 16101.1160\n",
      "Epoch [225/350], Loss: 15998.9742\n",
      "Epoch [226/350], Loss: 15866.9756\n",
      "Epoch [227/350], Loss: 15780.7852\n",
      "Epoch [228/350], Loss: 15723.4384\n",
      "Epoch [229/350], Loss: 15593.4621\n",
      "Epoch [230/350], Loss: 15499.1650\n",
      "Epoch [231/350], Loss: 15397.5731\n",
      "Epoch [232/350], Loss: 15337.5426\n",
      "Epoch [233/350], Loss: 15361.1859\n",
      "Epoch [234/350], Loss: 15118.3572\n",
      "Epoch [235/350], Loss: 15236.0129\n",
      "Epoch [236/350], Loss: 15080.4628\n",
      "Epoch [237/350], Loss: 15015.4155\n",
      "Epoch [238/350], Loss: 14889.3859\n",
      "Epoch [239/350], Loss: 14931.5421\n",
      "Epoch [240/350], Loss: 14774.3547\n",
      "Epoch [241/350], Loss: 14656.8712\n",
      "Epoch [242/350], Loss: 14641.8165\n",
      "Epoch [243/350], Loss: 14511.8967\n",
      "Epoch [244/350], Loss: 14390.7389\n",
      "Epoch [245/350], Loss: 14379.6322\n",
      "Epoch [246/350], Loss: 14321.1170\n",
      "Epoch [247/350], Loss: 14263.4610\n",
      "Epoch [248/350], Loss: 14241.8040\n",
      "Epoch [249/350], Loss: 14025.6492\n",
      "Epoch [250/350], Loss: 14083.9946\n",
      "Epoch [251/350], Loss: 13957.3218\n",
      "Epoch [252/350], Loss: 13940.1732\n",
      "Epoch [253/350], Loss: 13814.3705\n",
      "Epoch [254/350], Loss: 13750.4429\n",
      "Epoch [255/350], Loss: 13676.1924\n",
      "Epoch [256/350], Loss: 13543.1352\n",
      "Epoch [257/350], Loss: 13561.4116\n",
      "Epoch [258/350], Loss: 13426.7725\n",
      "Epoch [259/350], Loss: 13395.4474\n",
      "Epoch [260/350], Loss: 13347.0168\n",
      "Epoch [261/350], Loss: 13284.8525\n",
      "Epoch [262/350], Loss: 13164.2875\n",
      "Epoch [263/350], Loss: 13109.9670\n",
      "Epoch [264/350], Loss: 13084.2130\n",
      "Epoch [265/350], Loss: 12938.2573\n",
      "Epoch [266/350], Loss: 13068.6253\n",
      "Epoch [267/350], Loss: 12860.0492\n",
      "Epoch [268/350], Loss: 12863.0135\n",
      "Epoch [269/350], Loss: 12701.8640\n",
      "Epoch [270/350], Loss: 12686.0198\n",
      "Epoch [271/350], Loss: 12679.3168\n",
      "Epoch [272/350], Loss: 12574.0269\n",
      "Epoch [273/350], Loss: 12544.8532\n",
      "Epoch [274/350], Loss: 12463.7700\n",
      "Epoch [275/350], Loss: 12381.7089\n",
      "Epoch [276/350], Loss: 12321.8771\n",
      "Epoch [277/350], Loss: 12336.7197\n",
      "Epoch [278/350], Loss: 12230.0564\n",
      "Epoch [279/350], Loss: 12183.7086\n",
      "Epoch [280/350], Loss: 12116.1371\n",
      "Epoch [281/350], Loss: 12016.8587\n",
      "Epoch [282/350], Loss: 11940.1358\n",
      "Epoch [283/350], Loss: 11902.1666\n",
      "Epoch [284/350], Loss: 11861.1011\n",
      "Epoch [285/350], Loss: 11806.1380\n",
      "Epoch [286/350], Loss: 11767.7548\n",
      "Epoch [287/350], Loss: 11690.1924\n",
      "Epoch [288/350], Loss: 11668.7112\n",
      "Epoch [289/350], Loss: 11486.6709\n",
      "Epoch [290/350], Loss: 11594.3397\n",
      "Epoch [291/350], Loss: 11560.7993\n",
      "Epoch [292/350], Loss: 11397.0903\n",
      "Epoch [293/350], Loss: 11405.1064\n",
      "Epoch [294/350], Loss: 11322.9927\n",
      "Epoch [295/350], Loss: 11434.6213\n",
      "Epoch [296/350], Loss: 11239.4067\n",
      "Epoch [297/350], Loss: 11305.7689\n",
      "Epoch [298/350], Loss: 11110.8312\n",
      "Epoch [299/350], Loss: 11168.5995\n",
      "Epoch [300/350], Loss: 11035.4509\n",
      "Epoch [301/350], Loss: 11036.3687\n",
      "Epoch [302/350], Loss: 10922.1476\n",
      "Epoch [303/350], Loss: 10872.2661\n",
      "Epoch [304/350], Loss: 10856.4653\n",
      "Epoch [305/350], Loss: 10839.7776\n",
      "Epoch [306/350], Loss: 10672.3875\n",
      "Epoch [307/350], Loss: 10645.7421\n",
      "Epoch [308/350], Loss: 10641.7861\n",
      "Epoch [309/350], Loss: 10594.1359\n",
      "Epoch [310/350], Loss: 10563.3905\n",
      "Epoch [311/350], Loss: 10541.4725\n",
      "Epoch [312/350], Loss: 10546.7731\n",
      "Epoch [313/350], Loss: 10387.5070\n",
      "Epoch [314/350], Loss: 10356.6475\n",
      "Epoch [315/350], Loss: 10385.1427\n",
      "Epoch [316/350], Loss: 10266.5021\n",
      "Epoch [317/350], Loss: 10300.3675\n",
      "Epoch [318/350], Loss: 10197.3320\n",
      "Epoch [319/350], Loss: 10146.7767\n",
      "Epoch [320/350], Loss: 10122.2020\n",
      "Epoch [321/350], Loss: 10104.4695\n",
      "Epoch [322/350], Loss: 10012.6615\n",
      "Epoch [323/350], Loss: 9964.8185\n",
      "Epoch [324/350], Loss: 9907.8506\n",
      "Epoch [325/350], Loss: 9985.9969\n",
      "Epoch [326/350], Loss: 9843.8965\n",
      "Epoch [327/350], Loss: 9785.2659\n",
      "Epoch [328/350], Loss: 9740.9997\n",
      "Epoch [329/350], Loss: 9757.8945\n",
      "Epoch [330/350], Loss: 9718.4412\n",
      "Epoch [331/350], Loss: 9608.7269\n",
      "Epoch [332/350], Loss: 9537.6046\n",
      "Epoch [333/350], Loss: 9596.8698\n",
      "Epoch [334/350], Loss: 9537.0665\n",
      "Epoch [335/350], Loss: 9483.5244\n",
      "Epoch [336/350], Loss: 9478.3153\n",
      "Epoch [337/350], Loss: 9278.6913\n",
      "Epoch [338/350], Loss: 9403.7502\n",
      "Epoch [339/350], Loss: 9358.0127\n",
      "Epoch [340/350], Loss: 9392.2605\n",
      "Epoch [341/350], Loss: 9244.4425\n",
      "Epoch [342/350], Loss: 9217.4179\n",
      "Epoch [343/350], Loss: 9125.1159\n",
      "Epoch [344/350], Loss: 9175.2908\n",
      "Epoch [345/350], Loss: 9147.3956\n",
      "Epoch [346/350], Loss: 9084.0543\n",
      "Epoch [347/350], Loss: 9063.6302\n",
      "Epoch [348/350], Loss: 8982.1439\n",
      "Epoch [349/350], Loss: 8889.9944\n",
      "Epoch [350/350], Loss: 8986.9034\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "models = []\n",
    "trainers = []\n",
    "\n",
    "# beta_values = [0.1, 1, 10, 100, 500, 1000]\n",
    "beta_values = [200, 300, 500]\n",
    "for beta in beta_values:\n",
    "    model = VariationalAutoDecoder(latent_dim, device=device)\n",
    "    models.append(model)\n",
    "    trainer = VADTrainer(model=model, dl=train_dl, latent_dim=model.latent_dim, device=device, beta=beta)\n",
    "    trainers.append(trainer)\n",
    "    \n",
    "for trainer in trainers:\n",
    "    trainer.train(num_epochs=350)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD has finished test evaluation with a test loss of 0.17914448771625757.\n",
      "AD has finished train evaluation with a train loss of 0.18829396553337574.\n",
      "AD has finished test evaluation with a test loss of 0.18555865716189146.\n",
      "AD has finished train evaluation with a train loss of 0.19398182537406683.\n",
      "AD has finished test evaluation with a test loss of 0.1944260774180293.\n",
      "AD has finished train evaluation with a train loss of 0.20323046389967203.\n"
     ]
    }
   ],
   "source": [
    "###### Evaluation on the test dataset\n",
    "\n",
    "latents_lst = []\n",
    "for model in models:\n",
    "    mu_test = torch.randn(len(test_dl.dataset), latent_dim, device=device, requires_grad=True)\n",
    "    sigma_test = torch.randn(len(test_dl.dataset), latent_dim, device=device, requires_grad=True)\n",
    "    test_latents = torch.nn.parameter.Parameter(torch.stack([mu_test, sigma_test], dim=1)).to(device)\n",
    "    \n",
    "    opt = torch.optim.Adam([test_latents], lr=5e-3)\n",
    "    test_loss = evaluate_model(model=model, test_dl=test_dl, opt=opt, latents=test_latents, epochs=700, device=device)\n",
    "    latents_lst.append(test_latents)\n",
    "    print(f\"AD has finished test evaluation with a test loss of {test_loss}.\")\n",
    "\n",
    "    # Evaluation on the training dataset\n",
    "    mu_test = torch.randn(len(test_dl.dataset), latent_dim, device=device, requires_grad=True)\n",
    "    sigma_test = torch.randn(len(test_dl.dataset), latent_dim, device=device, requires_grad=True)\n",
    "    train_latents = torch.nn.parameter.Parameter(torch.stack([mu_test, sigma_test], dim=1)).to(device)\n",
    "    opt = torch.optim.Adam([train_latents], lr=1e-3)\n",
    "    train_loss = evaluate_model(model=model, test_dl=train_dl, opt=opt, latents=train_latents, epochs=700, device=device)\n",
    "    print(f\"AD has finished train evaluation with a train loss of {train_loss}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Vectors Shape: torch.Size([5, 2, 128])\n",
      "Random Vectors Shape: torch.Size([5, 128])\n",
      "Sampled Vectors Shape: torch.Size([5, 2, 128])\n",
      "Random Vectors Shape: torch.Size([5, 128])\n",
      "Sampled Vectors Shape: torch.Size([5, 2, 128])\n",
      "Random Vectors Shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 5 indices from the test dataset\n",
    "import random\n",
    "import utils\n",
    "\n",
    "\n",
    "random.seed(777)\n",
    "sampled_indices = random.sample(range(len(test_latents)), 5)\n",
    "\n",
    "samp = []\n",
    "for i, test_latents in enumerate(latents_lst):\n",
    "    # Extract the corresponding vectors (input data) and their labels\n",
    "    sampled_latents = [test_latents[i] for i in sampled_indices]  # Only selecting input data, not labels\n",
    "    \n",
    "    # Convert to a single tensor (optional)\n",
    "    sampled_latents_tensor = torch.stack(sampled_latents)\n",
    "    samp.append(sampled_latents_tensor)\n",
    "    \n",
    "    random_latents_tensor = torch.randn((5,128), device=device)\n",
    "    \n",
    "    print(\"Sampled Vectors Shape:\", sampled_latents_tensor.shape)  # Should be (5, *) depending on your data shape\n",
    "    print(\"Random Vectors Shape:\", random_latents_tensor.shape)  # Should be (5, *) depending on your data shape\n",
    "    \n",
    "    sampled_test_images = models[i](sampled_latents_tensor).view(-1, 1, 28, 28)\n",
    "    random_test_images = models[i].david_forward(random_latents_tensor).view(-1, 1, 28, 28)\n",
    "    \n",
    "    utils.save_images(sampled_test_images, f\"VAD_results/generated/sampled_test_images_{beta_values[i]}.png\")\n",
    "    utils.save_images(random_test_images, f\"VAD_results/generated/random_test_images_{beta_values[i]}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import plot_tsne\n",
    "\n",
    "for i,model in enumerate(models):\n",
    "    result_test_latents = model.reparameterization_trick(test_latents)\n",
    "    utils.plot_tsne(test_ds, result_test_latents, f\"VAD_results/tsne/tsne_test_{beta_values[i]}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "# #INTERPOLATION\n",
    "\n",
    "#INTERPOLATION CODE !!\n",
    "\n",
    "# need to check beforehand that the picked pictures will be of diffrenet classes\n",
    "sampled_latents = [result_test_latents[sampled_indices[0]],  result_test_latents[sampled_indices[1]]]\n",
    "\n",
    "#they asked for 5 differenet equally distrubed values, can probably do this using np but whatever\n",
    "# weights = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "weights = [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "interpolated_latents = [w * sampled_latents[0] + (1 - w) * sampled_latents[1] for w in weights]\n",
    "interpolated_latents_tensor = torch.stack(interpolated_latents)\n",
    "print(interpolated_latents_tensor.shape)\n",
    "interpolated_images = model.decoder(interpolated_latents_tensor).view(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "utils.save_images(interpolated_images, \"interpolted_image_normal_dist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
